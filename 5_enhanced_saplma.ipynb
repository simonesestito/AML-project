{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhMfodP7EdCj"
   },
   "source": [
    "# Trying to enhance SAPLMA\n",
    "In this notebook, we try to **improve the original SAPLMA architecture**.\n",
    "We test the new version of the classifier with various metrics (accuracy, precision, recall, ROC, AUC-ROC, confusion matrix) and observe some misclassified examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7riZzynJ_E9"
   },
   "source": [
    "# Imports, installations and declarations from previous notebooks\n",
    "\n",
    "This section can be skipped and collapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JXs-2fCJ_E-"
   },
   "outputs": [],
   "source": [
    "#@title Install missing dependencies\n",
    "!pip install wandb lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3U1bN2Y-J_E_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JH7N5rw6J_FA"
   },
   "outputs": [],
   "source": [
    "# If not in Colab, do some compatibility changes\n",
    "if not IN_COLAB:\n",
    "    DRIVE_PATH='.'\n",
    "    os.environ['HF_TOKEN'] = open('.hf_token').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fP6gb_MJ_FA",
    "outputId": "9c2a52a8-ffa4-4b03-9c7a-a1fad5ee930d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content\n",
      "artifacts  drive  hallucination_detector  publicDataset  sample_data  wandb\n",
      "\n",
      "HF_TOKEN found\n",
      "WANDB_API_KEY found and set as env var\n"
     ]
    }
   ],
   "source": [
    "#@title Mount Drive, if needed, and check the HF_TOKEN is set and accessible\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive, userdata\n",
    "\n",
    "    drive.mount('/content/drive', readonly=True)\n",
    "    DRIVE_PATH: str = '/content/drive/MyDrive/Final_Project/'\n",
    "    assert os.path.exists(DRIVE_PATH), 'Did you forget to create a shortcut in MyDrive named Final_Project this time as well? :('\n",
    "    !cp -R {DRIVE_PATH}/publicDataset .\n",
    "    !pwd\n",
    "    !ls\n",
    "    print()\n",
    "\n",
    "    assert userdata.get('HF_TOKEN'), 'Set up HuggingFace login secret properly in Colab!'\n",
    "    print('HF_TOKEN found')\n",
    "\n",
    "    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "    print('WANDB_API_KEY found and set as env var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYKSjI-DJ_FA",
    "outputId": "880305b5-56dd-4246-e1fe-1b24b3cf0a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_ecdsa  known_hosts\n",
      "Cloning into '/content/AML-project'...\n",
      "remote: Enumerating objects: 430, done.\u001b[K\n",
      "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
      "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
      "remote: Total 430 (delta 47), reused 59 (delta 30), pack-reused 338 (from 1)\u001b[K\n",
      "Receiving objects: 100% (430/430), 2.14 MiB | 5.29 MiB/s, done.\n",
      "Resolving deltas: 100% (232/232), done.\n"
     ]
    }
   ],
   "source": [
    "#@title Clone the new updated Python files from GitHub, from master\n",
    "if IN_COLAB:\n",
    "  !mkdir -p /root/.ssh\n",
    "  !touch /root/.ssh/id_ecdsa\n",
    "\n",
    "  with open('/root/.ssh/id_ecdsa', 'w') as f:\n",
    "    git_ssh_private_key = \"\"\"\n",
    "        -----BEGIN OPENSSH PRIVATE KEY-----\n",
    "        b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\n",
    "        QyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQAAAAJhAtW8YQLVv\n",
    "        GAAAAAtzc2gtZWQyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQA\n",
    "        AAAEA6ARNr020VevD7mkC4GFBVqlTcZP7hvn8B3xi5LDvzYIHdyU5p+Lp9loGBAI3b1NXI\n",
    "        ko1b95VFdPj8HgwHbxVAAAAAEHNpbW9uZUBhcmNobGludXgBAgMEBQ==\n",
    "        -----END OPENSSH PRIVATE KEY-----\n",
    "    \"\"\"\n",
    "    f.write('\\n'.join([line.strip() for line in git_ssh_private_key.split('\\n') if line.strip() ]) + '\\n')\n",
    "\n",
    "  with open('/root/.ssh/known_hosts', 'w') as f:\n",
    "    f.write(\"github.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\\n\")\n",
    "    f.write(\"github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\\n\")\n",
    "    f.write(\"github.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\\n\")\n",
    "\n",
    "  !chmod 400 ~/.ssh/id_ecdsa ~/.ssh/known_hosts\n",
    "  !ls ~/.ssh\n",
    "\n",
    "  # Clone the repository\n",
    "  !rm -rf /content/AML-project\n",
    "  !git clone git@github.com:simonesestito/AML-project.git /content/AML-project\n",
    "  assert os.path.exists('/content/AML-project/.git'), 'Error cloning the repository. See logs above for details'\n",
    "  !rm -rf ./hallucination_detector && mv /content/AML-project/hallucination_detector .\n",
    "  !rm -rf /content/AML-project  # We don't need the Git repo anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ly5uv1XmJ_FB"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport hallucination_detector\n",
    "import hallucination_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMraWvbuJ_FB"
   },
   "source": [
    "# Initialize Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XiCsNDlgJ_FC"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from hallucination_detector.llama import LlamaInstruct\n",
    "from hallucination_detector.dataset import StatementDataModule\n",
    "from hallucination_detector.extractor import LlamaHiddenStatesExtractor, WeightedMeanReduction, AttentionAwareWeightedMeanReduction\n",
    "from hallucination_detector.classifier import OriginalSAPLMAClassifier, LightningHiddenStateSAPLMA, EnhancedSAPLMAClassifier\n",
    "from hallucination_detector.utils import try_to_overfit, plot_weight_matrix\n",
    "from hallucination_detector.utils import ClassificatorEvaluation\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GEjB3pgZJ_FC"
   },
   "outputs": [],
   "source": [
    "llama = LlamaInstruct()\n",
    "assert not IN_COLAB or llama.device.type == 'cuda', 'The model should be running on a GPU. On CPU, it is impossible to run'\n",
    "\n",
    "if llama.device.type == 'cpu':\n",
    "    print('WARNING: You are running an LLM on the CPU. Beware of the long inference times! Use it ONLY FOR SMALL tests, like very small tests.', file=sys.stderr, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j15ra1ZD3S6s"
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-J8gNuJa9_UE",
    "outputId": "b654f77e-0789-46ac-be27-b25a1b11364a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: cities_true_false.csv\n",
      "Loading file: facts_true_false.csv\n",
      "Loading file: animals_true_false.csv\n",
      "Loading file: elements_true_false.csv\n",
      "Loading file: inventions_true_false.csv\n",
      "Loading file: companies_true_false.csv\n",
      "Loading file: generated_true_false.csv\n",
      "Found 6330 samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "datamodule = StatementDataModule(batch_size=batch_size, drive_path='publicDataset')\n",
    "datamodule.prepare_data()\n",
    "print(f'Found {len(datamodule.full_dataset)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26xSE7FZ-Hok",
    "outputId": "96677031-e849-43dd-f4e1-65a484bc514a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 6330\n",
      "Train dataset: 4868\n",
      "Val dataset: 1217\n",
      "Test dataset: 245\n"
     ]
    }
   ],
   "source": [
    "datamodule.set_test_topic('generated_true_false')\n",
    "datamodule.setup()\n",
    "print('Full dataset:', len(datamodule.full_dataset))\n",
    "print('Train dataset:', len(datamodule.train_dataset))\n",
    "print('Val dataset:', len(datamodule.val_dataset))\n",
    "print('Test dataset:', len(datamodule.test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fcRd-usN4HA0"
   },
   "outputs": [],
   "source": [
    "test_loader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piZAb7A2xOQw"
   },
   "source": [
    "# Test setups with Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tb-LPp9KGN_E"
   },
   "outputs": [],
   "source": [
    "already_existing_sweep_id_to_resume = ''  #@param {type: 'string'}\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'val/loss',\n",
    "        'goal': 'minimize',\n",
    "    },\n",
    "    'parameters': {\n",
    "        'hidden_states_layer_idx': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 3,   # From the 4th...\n",
    "            'max': 15,  # ...to the last one\n",
    "        },\n",
    "        'reduction': {\n",
    "            'values': ['mean', 'last'],\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'value': 64,\n",
    "        },\n",
    "        'lr': {\n",
    "            'value': 1e-5,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if already_existing_sweep_id_to_resume:\n",
    "  print('Resuming Sweep with ID', already_existing_sweep_id_to_resume)\n",
    "  sweep_id = already_existing_sweep_id_to_resume\n",
    "else:\n",
    "  sweep_id = wandb.sweep(sweep_config, entity='aml-2324-project', project='llama-hallucination-detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AducmHXBHNW3"
   },
   "outputs": [],
   "source": [
    "def train_model_with_config(config=None):\n",
    "  with wandb.init(config=config):\n",
    "    config = wandb.config\n",
    "    hidden_states_layer_idx, reduction, batch_size, lr = config.hidden_states_layer_idx, config.reduction, config.batch_size, config.lr\n",
    "\n",
    "    datamodule = StatementDataModule(batch_size=batch_size, drive_path='publicDataset')\n",
    "    datamodule.set_test_topic('generated_true_false')\n",
    "\n",
    "    enhanced_saplma = EnhancedSAPLMAClassifier()\n",
    "    model = LightningHiddenStateSAPLMA(llama, enhanced_saplma, hidden_states_layer_idx=hidden_states_layer_idx, lr=lr)\n",
    "    model.hparams.batch_size = batch_size\n",
    "\n",
    "    ## DA RIVEDERE\n",
    "\n",
    "    # Add WanbB logging + checkpoint saving\n",
    "    wandb_logger = WandbLogger(log_model='all', checkpoint_name=f'advanced-architecture-{wandb.run.id}')\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val/loss\", mode=\"min\", save_weights_only=True)  # Save checkpoint only if validation loss decreases\n",
    "    # A few epochs are fine to understand the main differences between the configurations\n",
    "    trainer = pl.Trainer(max_epochs=3, log_every_n_steps=10, logger=wandb_logger, callbacks=[checkpoint_callback])\n",
    "    trainer.fit(model=model, datamodule=datamodule)\n",
    "    trainer.test(model=model, datamodule=datamodule)\n",
    "\n",
    "wandb.agent(sweep_id, train_model_with_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZzSfjmq-KDg"
   },
   "outputs": [],
   "source": [
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORXFFhXGR0re"
   },
   "outputs": [],
   "source": [
    "reduction1 = WeightedMeanReduction(num_layers = 16, num_tokens = 70)\n",
    "reduction2 = AttentionAwareWeightedMeanReduction(num_layers = 16, num_tokens = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHuapr5fG7Gt"
   },
   "outputs": [],
   "source": [
    "for reduction in [reduction1, reduction2]:\n",
    "  enhanced_saplma = EnhancedSAPLMAClassifier()\n",
    "  model = LightningHiddenStateSAPLMA(llama, enhanced_saplma, reduction, lr=lr)\n",
    "  model.hparams.batch_size = batch_size\n",
    "\n",
    "  ## DA RIVEDERE\n",
    "\n",
    "  # Add WanbB logging + checkpoint saving\n",
    "  wandb_logger = WandbLogger(log_model='all', checkpoint_name=f'weighted-architecture-{wandb.run.id}')\n",
    "  checkpoint_callback = ModelCheckpoint(monitor=\"val/loss\", mode=\"min\", save_weights_only=True)  # Save checkpoint only if validation loss decreases\n",
    "  # A few epochs are fine to understand the main differences between the configurations\n",
    "  trainer = pl.Trainer(max_epochs=32, log_every_n_steps=10, logger=wandb_logger, callbacks=[checkpoint_callback])\n",
    "  trainer.fit(model=model, datamodule=datamodule)\n",
    "  trainer.test(model=model, datamodule=datamodule)\n",
    "\n",
    "  weight_matrix = F.softmax(model.reduction.weight_matrix.detach(), dim=0).cpu().view((16, 70)).numpy()\n",
    "  plot_weight_matrix(weight_matrix)\n",
    "\n",
    "  tokens_only_weights = model.reduction.weight_matrix.detach().cpu().view((16, 70)).sum(dim=0)\n",
    "  assert tokens_only_weights.shape == (70,), f'Expected shape (70,), got {tokens_only_weights.shape}'\n",
    "  tokens_only_weights = F.softmax(tokens_only_weights, dim=0)\n",
    "  assert tokens_only_weights.shape == (70,), f'Expected shape (70,), got {tokens_only_weights.shape}'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "I7riZzynJ_E9",
    "SMraWvbuJ_FB",
    "j15ra1ZD3S6s"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
