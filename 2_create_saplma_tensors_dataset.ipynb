{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Create pre-computed dataset\n",
        "\n",
        "We have observed that the output and the hidden layers of the LLM are deterministic.\n",
        "\n",
        "In this notebook we aim to create a dataset of precomputed tensors (hidden states, activations, ...), persistently saved on disk."
      ],
      "metadata": {
        "id": "iFdTkwSE0l15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports, installations and declarations from previous notebooks\n",
        "\n",
        "This section can be skipped and collapsed."
      ],
      "metadata": {
        "id": "uXknG8gh2l6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Drive\n",
        "import os\n",
        "\n",
        "# Make this try/except to let this notebook work on Drive but also locally\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  DRIVE_PATH = '/content/drive/MyDrive/Final_Project/'\n",
        "  assert os.path.exists(DRIVE_PATH), 'Did you forget to create a shortcut in MyDrive named Final_Project this time as well? :('\n",
        "except ModuleNotFoundError:\n",
        "  DRIVE_PATH = '.'\n",
        "  assert os.path.abspath(os.getcwd()).split(os.path.sep)[-1] == 'Final_Project'\n",
        "\n",
        "%cd {DRIVE_PATH}\n",
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJq0xkuVyLFx",
        "outputId": "df92cae8-32b4-4dfc-e76a-697794095345",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/.shortcut-targets-by-id/1WdIP20OinXKeEN_xVOHEa6WVcY4eSO-k/Final_Project\n",
            "/content/drive/.shortcut-targets-by-id/1WdIP20OinXKeEN_xVOHEa6WVcY4eSO-k/Final_Project\n",
            "1_experiments_on_llama_and_saplma.ipynb  publicDataset\n",
            "2_create_saplma_tensors_dataset.ipynb\t saplma-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SZa9_tD7vEJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b88559-a026-4de9-ca0c-c20561146df2",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: transformers>=4.36 in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36) (0.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "#@title Install dependencies\n",
        "\n",
        "# PyTorch (CPU only, if not installed yet)\n",
        "try:\n",
        "    import torch\n",
        "except ModuleNotFoundError:\n",
        "    !pip install 'torch>=2.1.1' torchvision --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# Huggingface dependencies\n",
        "!pip install huggingface-hub 'transformers>=4.36' 'accelerate>=0.26.0'\n",
        "\n",
        "# Visualization dependencies\n",
        "!pip install matplotlib seaborn ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a92JSycqlAAn",
        "outputId": "953f8c86-3d40-4945-bbcb-b8eac1abcb90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found HF_TOKEN in Colab secrets\n"
          ]
        }
      ],
      "source": [
        "#@title Initialize the secret for HuggingFace login\n",
        "import os\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    # We are in colab, so we should access it from userdata.get(...)\n",
        "    assert userdata.get('HF_TOKEN'), 'Set up HuggingFace login secret properly in Colab!'\n",
        "    print('Found HF_TOKEN in Colab secrets')\n",
        "except ModuleNotFoundError:\n",
        "    # Not in colab, so we have to setup the token manually reading from a file\n",
        "    if os.getenv('HF_TOKEN'):\n",
        "        print('Found HF_TOKEN in environment variables')\n",
        "    else:\n",
        "        # Read it from a file\n",
        "        hf_token_file = '.hf_token'\n",
        "        assert os.path.exists(hf_token_file), f'You must create a file in this working directory ({os.getcwd()}) called {hf_token_file}, containing the Huggingface personal secret access token'\n",
        "        with open(hf_token_file, 'r') as f:\n",
        "            os.environ['HF_TOKEN'] = f.read().strip()\n",
        "            print('Found HF_TOKEN in file')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-declare the Llama abstractions we made\n",
        "\n",
        "From notebook **1**"
      ],
      "metadata": {
        "id": "NiIYnSrS3BdI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1IOdwRgjkQvO"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from collections.abc import Iterator\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.set_default_dtype(torch.float16)\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\" #info at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dx6Gpm1LHyIJ"
      },
      "outputs": [],
      "source": [
        "class LlamaPrompt:\n",
        "  \"\"\"\n",
        "   Class to represent a prompt for the Llama model, which is made of a system prompt,\n",
        "   which sets the general context according to which the AI should respond,\n",
        "   and a user prompt, which is the text that the AI should respond to.\n",
        "  \"\"\"\n",
        "  user_prompt: str\n",
        "  system_prompt: str\n",
        "\n",
        "  def __init__(self, user_prompt, system_prompt=\"You are a helpful AI assistant.\"):\n",
        "    self.user_prompt = user_prompt\n",
        "    self.system_prompt = system_prompt\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "      # From: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/#-instruct-model-prompt-\n",
        "      return ''.join([\n",
        "          \"<|begin_of_text|>\",\n",
        "          f\"<|start_header_id|>system<|end_header_id|>{self.system_prompt}<|eot_id|>\",\n",
        "          f\"<|start_header_id|>user<|end_header_id|>{self.user_prompt}<|eot_id|>\",\n",
        "          \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "      ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y7u7XD1pK7Kf"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class LlamaResponse:\n",
        "    \"\"\"\n",
        "      Class to represent a response given by the Llama model.\n",
        "    \"\"\"\n",
        "    prompt: LlamaPrompt\n",
        "    response: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iDVTcnv9KYmo"
      },
      "outputs": [],
      "source": [
        "class LlamaInstruct:\n",
        "    \"\"\"\n",
        "    Class to wrap the Llama model methods for ease of usage\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str, model_args: dict = None, tokenizer_args: dict = None, pad_token: str = None):\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.model_args = model_args if model_args is not None else dict()\n",
        "        self.tokenizer_args = tokenizer_args if tokenizer_args is not None else dict()\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\", **self.model_args)\n",
        "        self.model.eval()\n",
        "        self.device = self.model.device\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left', **self.tokenizer_args)\n",
        "        self.pad_token = self.tokenizer.eos_token if pad_token is None else pad_token\n",
        "        self.tokenizer.pad_token = self.pad_token\n",
        "\n",
        "        self.assistant_header = self.tokenizer.encode(\"<|start_header_id|>assistant<|end_header_id|>\", return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        self.registered_hooks = []\n",
        "\n",
        "    # to tokenize input prompts\n",
        "    def tokenize(self, prompts: str | LlamaPrompt | list[str | LlamaPrompt], pad_to_max_length: int = 70) -> tuple[dict, list[LlamaPrompt]]:\n",
        "\n",
        "        # Make prompts a list anyway\n",
        "        if not isinstance(prompts, list):\n",
        "            prompts = [ prompts ]\n",
        "\n",
        "        # Convert all prompts to LlamaPrompt\n",
        "        prompts = [ prompt if isinstance(prompt, LlamaPrompt) else LlamaPrompt(prompt) for prompt in prompts ]\n",
        "\n",
        "        # tokenizer output will be a dictionary of pytorch tensors with keys \"input_ids\" (numerical ids of tokens)\n",
        "        # and \"attention_mask\" (1 for actual input tokens and 0 for padding tokens)\n",
        "        inputs = self.tokenizer(\n",
        "            [ str(prompt) for prompt in prompts ],\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length',\n",
        "            max_length=pad_to_max_length,\n",
        "        ).to(self.device)\n",
        "\n",
        "        return inputs, prompts\n",
        "\n",
        "    # to make Llama generate responses\n",
        "    def generate(self, inputs: dict, generate_args: dict = None): #-> Iterator[LlamaResponse] ? right now it does not return that\n",
        "\n",
        "        generate_args = generate_args if generate_args is not None else dict()\n",
        "        default_args = {\n",
        "            \"max_length\": 100,\n",
        "            \"num_return_sequences\": 1,\n",
        "            \"temperature\": 0.1,\n",
        "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
        "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
        "        }\n",
        "\n",
        "        # Overwrite default_args with generate_args\n",
        "        default_args.update(generate_args)\n",
        "\n",
        "        # returns (batch_size, sequence_length) tensors with token ids of the generated response, including input tokens\n",
        "        return self.model.generate(\n",
        "            **inputs,\n",
        "            **default_args,\n",
        "        )\n",
        "\n",
        "    # to extract Llama answers as decoded text in a LlamaResponse object starting from encoded input\n",
        "    def extract_responses(self, input_ids: torch.Tensor, outputs: torch.Tensor, prompts: list[LlamaPrompt]) -> Iterator[LlamaResponse]:\n",
        "\n",
        "        for input, output, prompt in zip(input_ids, outputs, prompts):\n",
        "            # Remove the prompt from the output generated\n",
        "            output = output[len(input):]\n",
        "\n",
        "            # Remove another assistant_header, if present\n",
        "            if torch.equal(output[:len(self.assistant_header)], self.assistant_header):\n",
        "                output = output[len(self.assistant_header):]\n",
        "\n",
        "            generated = self.tokenizer.decode(output, skip_special_tokens=True).strip()\n",
        "\n",
        "            yield LlamaResponse(prompt, generated)\n",
        "\n",
        "    # to get textual Llama responses starting from textual prompts\n",
        "    def run(self, prompts: str | LlamaPrompt | list[str | LlamaPrompt], verbose: bool = False) -> Iterator[LlamaResponse]:\n",
        "\n",
        "        # Optional logging function\n",
        "        def _print(*args, **kwargs):\n",
        "            if verbose:\n",
        "                print(*args, **kwargs)\n",
        "\n",
        "        inputs, prompts = self.tokenize(prompts)\n",
        "\n",
        "        _print('Tokenized inputs:', inputs.input_ids.shape)\n",
        "        _print('Last tokens:', inputs.input_ids[:, -1])\n",
        "\n",
        "        outputs = self.generate(inputs)\n",
        "        _print('Generated outputs:', outputs.shape)\n",
        "\n",
        "        return self.extract_responses(inputs.input_ids, outputs, prompts)\n",
        "\n",
        "    # an hook is a piece of customized code to be run in the forward or backward pass of a model\n",
        "    # (useful for debugging)\n",
        "    def register_hook(self, module, hook_fn):\n",
        "        '''\n",
        "        Register a hook, in such a way that we have a very easy way to remove the hook later.\n",
        "\n",
        "        Example usage:\n",
        "\n",
        "        llama.unregister_all_hooks()\n",
        "        for module_name, module in llama.model.named_modules():\n",
        "            if something():\n",
        "                llama.register_hook(module, hook_fn)\n",
        "        '''\n",
        "        handle = module.register_forward_hook(hook_fn)\n",
        "        self.registered_hooks.append(handle)\n",
        "\n",
        "    def unregister_all_hooks(self):\n",
        "        '''\n",
        "        Remove all of our registered hooks.\n",
        "        '''\n",
        "        for handle in self.registered_hooks:\n",
        "            handle.remove()\n",
        "\n",
        "    def _get_model_num_heads(self) -> int:\n",
        "        return self.model.config.num_attention_heads\n",
        "\n",
        "    def _get_model_hidden_layers(self) -> int:\n",
        "        return self.model.config.num_hidden_layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lGmDcnZQMpBe"
      },
      "outputs": [],
      "source": [
        "llama = LlamaInstruct(model_name, model_args={\"attn_implementation\": \"eager\"})\n",
        "assert llama.device.type == 'cuda', 'The model should be running on a GPU. On CPU, it is impossible to run'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation\n",
        "\n",
        "From the SAPLMA dataset, run Llama in inference mode and save, for each input sequence:\n",
        "- the string itself\n",
        "- its topic\n",
        "- its label class (true / false)\n",
        "- all hidden states, for each layer\n",
        "- all attention maps, for each layer\n",
        "\n",
        "---\n",
        "\n",
        "It will be organized on disk in the following way.\n",
        "\n",
        "`./saplma-data/{topic_name}/{true-false}/` directory containing:\n",
        "- `strings.txt` = file of all strings in that topic, that are all true or false, according to the parent folder's name\n",
        "- `[index]_hidden_states.pt` = all hidden states for the i-th string\n",
        "- `[index]_attention_maps.pt` = all attention maps for the i-th string"
      ],
      "metadata": {
        "id": "LOJwlMrW3Upj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load SAPLMA dataset\n",
        "class StatementDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for statements and their truth values.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): The combined dataset from all CSV files.\n",
        "                                      Expects columns ['statement', 'label', 'topic'].\n",
        "        \"\"\"\n",
        "        self.data = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int): Index of the row to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (statement, label, topic), where statement is the text,\n",
        "                   label is the binary target, and topic is the source file name.\n",
        "        \"\"\"\n",
        "        row = self.data.iloc[idx]\n",
        "        statement = row['statement']\n",
        "        label = torch.tensor(row['label'])\n",
        "        topic = row['topic']\n",
        "        return statement, label, topic\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset_with_topics(drive_path):\n",
        "    \"\"\"\n",
        "    Create a StatementDataset from CSV files in a specified Google Drive folder,\n",
        "    adding a 'topic' column to indicate the source file of each row.\n",
        "\n",
        "    Args:\n",
        "        drive_path (str): Path to the folder containing the CSV files.\n",
        "\n",
        "    Returns:\n",
        "        StatementDataset: PyTorch Dataset for the combined dataset.\n",
        "    \"\"\"\n",
        "    # Ensure the path exists\n",
        "    if not os.path.exists(drive_path):\n",
        "        raise ValueError(f\"Path '{drive_path}' does not exist.\")\n",
        "\n",
        "    all_dataframes = []\n",
        "    for file_name in os.listdir(drive_path):\n",
        "        file_path = os.path.join(drive_path, file_name)\n",
        "        if file_name.endswith(\".csv\"):\n",
        "            print(f\"Loading file: {file_name}\")\n",
        "            # Read the CSV and add a 'topic' column with the file name (without extension)\n",
        "            df = pd.read_csv(file_path)\n",
        "            df['topic'] = os.path.splitext(file_name)[0]  # Add topic column\n",
        "            all_dataframes.append(df)\n",
        "\n",
        "    if all_dataframes:\n",
        "        combined_dataset = pd.concat(all_dataframes, ignore_index=True)\n",
        "    else:\n",
        "        raise ValueError(f\"No CSV files found in the directory '{drive_path}'.\")\n",
        "\n",
        "    # Create and return the PyTorch Dataset\n",
        "    return StatementDataset(combined_dataset)\n",
        "\n",
        "\n",
        "\n",
        "dataset_path = os.path.join(DRIVE_PATH, \"publicDataset\")\n",
        "assert os.path.exists(dataset_path)\n",
        "dataset = create_dataset_with_topics(dataset_path)\n",
        "print(f'Got {len(dataset)} samples.')\n",
        "\n",
        "topics = dataset.data['topic'].unique()\n",
        "print(f\"Topics: {topics}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ6qGzfT32RC",
        "outputId": "dc5e51f2-e2b0-43d9-ce61-b15c7ffa251c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading file: cities_true_false.csv\n",
            "Loading file: animals_true_false.csv\n",
            "Loading file: elements_true_false.csv\n",
            "Loading file: inventions_true_false.csv\n",
            "Loading file: companies_true_false.csv\n",
            "Loading file: generated_true_false.csv\n",
            "Loading file: facts_true_false.csv\n",
            "Got 6330 samples.\n",
            "Topics: ['cities_true_false' 'animals_true_false' 'elements_true_false'\n",
            " 'inventions_true_false' 'companies_true_false' 'generated_true_false'\n",
            " 'facts_true_false']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create the separated strings.txt\n",
        "\n",
        "SAPLMA_DATA_PATH = os.path.join(DRIVE_PATH, \"saplma-data\")\n",
        "os.makedirs(SAPLMA_DATA_PATH, exist_ok=True)\n",
        "\n",
        "TRUE_DIR_NAME, FALSE_DIR_NAME, STRINGS_TXT_NAME = \"true\", \"false\", \"strings.txt\"\n",
        "\n",
        "for topic in topics:\n",
        "  # Create topic folder\n",
        "  topic_path = os.path.join(SAPLMA_DATA_PATH, topic)\n",
        "  os.makedirs(topic_path, exist_ok=True)\n",
        "\n",
        "  # Create true/false subfolders\n",
        "  true_path = os.path.join(topic_path, TRUE_DIR_NAME)\n",
        "  os.makedirs(true_path, exist_ok=True)\n",
        "\n",
        "  false_path = os.path.join(topic_path, FALSE_DIR_NAME)\n",
        "  os.makedirs(false_path, exist_ok=True)\n",
        "\n",
        "  # Filter the strings of this topic\n",
        "  samples_per_topic = dataset.data[dataset.data['topic'] == topic]\n",
        "\n",
        "  # Append them into associated text files\n",
        "  with open(os.path.join(true_path, STRINGS_TXT_NAME), 'w') as f_true, open(os.path.join(false_path, STRINGS_TXT_NAME), 'w') as f_false:\n",
        "    for index, row in samples_per_topic.iterrows():\n",
        "      statement = row['statement']\n",
        "      label = row['label']\n",
        "      if label == 1:\n",
        "        f_true.write(statement + '\\n')\n",
        "      elif label == 0:\n",
        "        f_false.write(statement + '\\n')\n",
        "      else:\n",
        "        raise ValueError(f\"Invalid label value: {label}\")\n",
        "\n",
        "  # Count the lines of the 2 strings files\n",
        "  with open(os.path.join(true_path, STRINGS_TXT_NAME), 'r') as f_true, open(os.path.join(false_path, STRINGS_TXT_NAME), 'r') as f_false:\n",
        "    true_lines = len(f_true.readlines())\n",
        "    false_lines = len(f_false.readlines())\n",
        "\n",
        "    print(f'Topic {topic:>25}: {true_lines:>4d} true, {false_lines:>4d} false')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLkPefGO5KLd",
        "outputId": "b632d0d4-40b3-4624-9cb4-c3faee053e11"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic         cities_true_false:  729 true,  729 false\n",
            "Topic        animals_true_false:  504 true,  504 false\n",
            "Topic       elements_true_false:  465 true,  465 false\n",
            "Topic     inventions_true_false:  464 true,  412 false\n",
            "Topic      companies_true_false:  600 true,  600 false\n",
            "Topic      generated_true_false:  119 true,  126 false\n",
            "Topic          facts_true_false:  306 true,  307 false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run the LLM, collecting hidden_states\n",
        "@torch.no_grad()\n",
        "def extract_hidden_states(strings: list[str], max_new_tokens: int = 5, num_return_sequences: int = 1) -> torch.Tensor:\n",
        "    tokenized_strings, _ = llama.tokenize(strings, pad_to_max_length=70)\n",
        "\n",
        "    # Check that every string ends with the padding token = the string wouldn't have required a bigger max_length in the tokenizer\n",
        "    padding_final_token_id = 128007\n",
        "    assert torch.all(tokenized_strings.input_ids[:, -1] == padding_final_token_id), 'Every string should end with the padding token'\n",
        "\n",
        "    outputs = llama.generate(\n",
        "      tokenized_strings,\n",
        "      generate_args={\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"max_length\": None,\n",
        "        \"num_return_sequences\": num_return_sequences,\n",
        "        \"output_hidden_states\": True,  # Make sure to output hidden states\n",
        "        \"output_attentions\": True,  # Make sure to output attention maps\n",
        "        \"return_dict_in_generate\": True,\n",
        "      }\n",
        "    )\n",
        "\n",
        "    hidden_states, attentions = outputs.hidden_states, outputs.attentions\n",
        "\n",
        "    # Elaborate attentions\n",
        "    # We are interested in the attentions with respect to the input tokens: they are in the first element in attentions\n",
        "    input_attentions_all_layers = attentions[0]  # Now, it is a tuple[16 = layers_num] of Tensor[batch_size, num_heads, input_tokens, input_tokens]\n",
        "    print(f'input_attentions_all_layers: {type(input_attentions_all_layers)} of {len(input_attentions_all_layers)} {input_attentions_all_layers[0].shape}')\n",
        "\n",
        "    # Elaborate hidden_states\n",
        "    # We are interested in the forward pass with only the input sequence, and no output yet.\n",
        "    # This is the first hidden_state\n",
        "    input_hidden_state_all_layers = hidden_states[0]  # Now, it is a tuple[17 = embedding + layers_num] of Tensor[batch_size, input_tokens, token_dim = 2048]\n",
        "    print(f'input_hidden_state_all_layers: {type(input_hidden_state_all_layers)} of {len(input_hidden_state_all_layers)} {input_hidden_state_all_layers[0].shape}')\n",
        "\n",
        "    return input_attentions_all_layers, input_hidden_state_all_layers\n",
        "\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "for topic in topics:\n",
        "  topic_path = os.path.join(SAPLMA_DATA_PATH, topic)\n",
        "  assert os.path.exists(topic_path), f'Folder for topic {topic} does not exist'\n",
        "\n",
        "  for subdir_name in [TRUE_DIR_NAME, FALSE_DIR_NAME]:\n",
        "    subdir_path = os.path.join(topic_path, subdir_name)\n",
        "    assert os.path.exists(subdir_path), f'Subfolder {subdir_path} does not exist'\n",
        "\n",
        "    strings_file = os.path.join(subdir_path, STRINGS_TXT_NAME)\n",
        "    assert os.path.exists(strings_file), f'Strings file {strings_file} does not exist'\n",
        "\n",
        "    with open(strings_file, 'r') as f:\n",
        "      strings = [s.strip() for s in f.readlines() if s.strip()]\n",
        "\n",
        "    print(f'Topic {topic:>25}: {len(strings):>4d} {subdir_name}')\n",
        "\n",
        "    for batch_i in range(0, len(strings), batch_size):\n",
        "      batch_strings = strings[batch_i:batch_i + batch_size]\n",
        "      print(f'\\tbatch_i={batch_i} [{batch_i}:{batch_i+len(batch_strings)}]')\n",
        "\n",
        "      input_attentions_all_layers, input_hidden_state_all_layers = extract_outputs(strings)\n",
        "\n",
        "      # Save each string index on its own\n",
        "      for i in range(len(batch_strings)):\n",
        "        input_attentions_all_layers_i = input_attentions_all_layers[i]  # Tensor[num_heads = 32, input_tokens = 70, input_tokens = 70]\n",
        "        input_hidden_state_all_layers_i = input_hidden_state_all_layers[i]  # Tensor[input_tokens = 70, token_dim = 2048]\n",
        "\n",
        "        # Save them into a file\n",
        "        string_global_idx = batch_i + i  # This is the absolute index that the i-th string in this batch has\n",
        "        attn_idx_file = os.path.join(subdir_path, f'{string_global_idx}_attention_maps.pt')\n",
        "        hidden_idx_file = os.path.join(subdir_path, f'{batch_i + i}_hidden_states.pt')\n",
        "        torch.save(input_attentions_all_layers_i, attn_idx_file)\n",
        "        torch.save(input_hidden_state_all_layers_i, hidden_idx_file)\n",
        "\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "Qk2Br4Kr7RWY",
        "outputId": "0f42d245-e00f-4381-8da2-d43061bc7a96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic         cities_true_false:  729 true\n",
            "\tbatch_i=0 [0:4]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 714.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 535.06 MiB is free. Process 365816 has 14.22 GiB memory in use. Of the allocated memory 12.50 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d7137278dc40>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\tbatch_i={batch_i} [{batch_i}:{batch_i+len(batch_strings)}]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m       \u001b[0minput_attentions_all_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_hidden_state_all_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;31m# Save each string index on its own\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d7137278dc40>\u001b[0m in \u001b[0;36mextract_outputs\u001b[0;34m(strings, max_new_tokens, num_return_sequences)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_strings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpadding_final_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Every string should end with the padding token'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     outputs = llama.generate(\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0mtokenized_strings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       generate_args={\n",
            "\u001b[0;32m<ipython-input-7-de7372ee3636>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generate_args)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# returns (batch_size, sequence_length) tensors with token ids of the generated response, including input tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         return self.model.generate(\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mdefault_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3222\u001b[0m             \u001b[0;31m# pre-process distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3223\u001b[0;31m             \u001b[0mnext_token_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3225\u001b[0m             \u001b[0;31m# Store scores, attentions and hidden_states when required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0madd_start_docstrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOGITS_PROCESSOR_INPUTS_DOCSTRING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0msorted_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0mcumulative_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 714.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 535.06 MiB is free. Process 365816 has 14.22 GiB memory in use. Of the allocated memory 12.50 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run the LLM, collecting attentions\n",
        "@torch.no_grad()\n",
        "def extract_attentions(strings: list[str], max_new_tokens: int = 5, num_return_sequences: int = 1) -> torch.Tensor:\n",
        "    tokenized_strings, _ = llama.tokenize(strings, pad_to_max_length=70)\n",
        "\n",
        "    # Check that every string ends with the padding token = the string wouldn't have required a bigger max_length in the tokenizer\n",
        "    padding_final_token_id = 128007\n",
        "    assert torch.all(tokenized_strings.input_ids[:, -1] == padding_final_token_id), 'Every string should end with the padding token'\n",
        "\n",
        "    outputs = llama.generate(\n",
        "      tokenized_strings,\n",
        "      generate_args={\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"max_length\": None,\n",
        "        \"num_return_sequences\": num_return_sequences,\n",
        "        \"output_hidden_states\": True,  # Make sure to output hidden states\n",
        "        \"output_attentions\": True,  # Make sure to output attention maps\n",
        "        \"return_dict_in_generate\": True,\n",
        "      }\n",
        "    )\n",
        "\n",
        "    hidden_states, attentions = outputs.hidden_states, outputs.attentions\n",
        "\n",
        "    # Elaborate attentions\n",
        "    # We are interested in the attentions with respect to the input tokens: they are in the first element in attentions\n",
        "    input_attentions_all_layers = attentions[0]  # Now, it is a tuple[16 = layers_num] of Tensor[batch_size, num_heads, input_tokens, input_tokens]\n",
        "    print(f'input_attentions_all_layers: {type(input_attentions_all_layers)} of {len(input_attentions_all_layers)} {input_attentions_all_layers[0].shape}')\n",
        "\n",
        "    # Elaborate hidden_states\n",
        "    # We are interested in the forward pass with only the input sequence, and no output yet.\n",
        "    # This is the first hidden_state\n",
        "    input_hidden_state_all_layers = hidden_states[0]  # Now, it is a tuple[17 = embedding + layers_num] of Tensor[batch_size, input_tokens, token_dim = 2048]\n",
        "    print(f'input_hidden_state_all_layers: {type(input_hidden_state_all_layers)} of {len(input_hidden_state_all_layers)} {input_hidden_state_all_layers[0].shape}')\n",
        "\n",
        "    return input_attentions_all_layers, input_hidden_state_all_layers\n",
        "\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "for topic in topics:\n",
        "  topic_path = os.path.join(SAPLMA_DATA_PATH, topic)\n",
        "  assert os.path.exists(topic_path), f'Folder for topic {topic} does not exist'\n",
        "\n",
        "  for subdir_name in [TRUE_DIR_NAME, FALSE_DIR_NAME]:\n",
        "    subdir_path = os.path.join(topic_path, subdir_name)\n",
        "    assert os.path.exists(subdir_path), f'Subfolder {subdir_path} does not exist'\n",
        "\n",
        "    strings_file = os.path.join(subdir_path, STRINGS_TXT_NAME)\n",
        "    assert os.path.exists(strings_file), f'Strings file {strings_file} does not exist'\n",
        "\n",
        "    with open(strings_file, 'r') as f:\n",
        "      strings = [s.strip() for s in f.readlines() if s.strip()]\n",
        "\n",
        "    print(f'Topic {topic:>25}: {len(strings):>4d} {subdir_name}')\n",
        "\n",
        "    for batch_i in range(0, len(strings), batch_size):\n",
        "      batch_strings = strings[batch_i:batch_i + batch_size]\n",
        "      print(f'\\tbatch_i={batch_i} [{batch_i}:{batch_i+len(batch_strings)}]')\n",
        "\n",
        "      input_attentions_all_layers, input_hidden_state_all_layers = extract_outputs(strings)\n",
        "\n",
        "      # Save each string index on its own\n",
        "      for i in range(len(batch_strings)):\n",
        "        input_attentions_all_layers_i = input_attentions_all_layers[i]  # Tensor[num_heads = 32, input_tokens = 70, input_tokens = 70]\n",
        "        input_hidden_state_all_layers_i = input_hidden_state_all_layers[i]  # Tensor[input_tokens = 70, token_dim = 2048]\n",
        "\n",
        "        # Save them into a file\n",
        "        string_global_idx = batch_i + i  # This is the absolute index that the i-th string in this batch has\n",
        "        attn_idx_file = os.path.join(subdir_path, f'{string_global_idx}_attention_maps.pt')\n",
        "        hidden_idx_file = os.path.join(subdir_path, f'{batch_i + i}_hidden_states.pt')\n",
        "        torch.save(input_attentions_all_layers_i, attn_idx_file)\n",
        "        torch.save(input_hidden_state_all_layers_i, hidden_idx_file)\n",
        "\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "f3VAqQ2bQmKc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}