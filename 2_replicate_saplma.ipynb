{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0aJqYFbJ_E6"
      },
      "source": [
        "# Replicate SAPLMA\n",
        "In this notebook, we aim to replicate what the original paper has done, in order to have a working baseline.\n",
        "\n",
        "Source: https://arxiv.org/pdf/2304.13734"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7riZzynJ_E9"
      },
      "source": [
        "## Imports, installations and declarations from previous notebooks\n",
        "\n",
        "This section can be skipped and collapsed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JXs-2fCJ_E-",
        "outputId": "57410526-a964-435d-ef7f-878b8375c56d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Collecting lightning\n",
            "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.10.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.6)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
            "Downloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.4.0 lightning-utilities-0.11.9 pytorch-lightning-2.4.0 torchmetrics-1.6.0\n"
          ]
        }
      ],
      "source": [
        "#@title Install missing dependencies\n",
        "!pip install wandb lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3U1bN2Y-J_E_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JH7N5rw6J_FA"
      },
      "outputs": [],
      "source": [
        "# If not in Colab, do some compatibility changes\n",
        "if not IN_COLAB:\n",
        "    DRIVE_PATH='.'\n",
        "    os.environ['HF_TOKEN'] = open('.hf_token').read().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fP6gb_MJ_FA",
        "outputId": "a19b0e53-dce2-4eeb-b8bb-e24b87cb20ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1WdIP20OinXKeEN_xVOHEa6WVcY4eSO-k/Final_Project\n",
            " 1_experiments_on_llama_and_saplma.ipynb   lightning_logs\n",
            " 2_replicate_saplma.ipynb\t\t   publicDataset\n",
            "'AML - First presentation.gslides'\t   X_create_saplma_tensors_dataset.ipynb\n",
            " hallucination_detector\n",
            "\n",
            "HF_TOKEN found\n",
            "WANDB_API_KEY found and set as env var\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Drive, if needed, and check the HF_TOKEN is set and accessible\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive, userdata\n",
        "\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_PATH: str = '/content/drive/MyDrive/Final_Project/'\n",
        "    assert os.path.exists(DRIVE_PATH), 'Did you forget to create a shortcut in MyDrive named Final_Project this time as well? :('\n",
        "    %cd {DRIVE_PATH}\n",
        "    !ls\n",
        "    print()\n",
        "\n",
        "    assert userdata.get('HF_TOKEN'), 'Set up HuggingFace login secret properly in Colab!'\n",
        "    print('HF_TOKEN found')\n",
        "\n",
        "    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "    print('WANDB_API_KEY found and set as env var')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYKSjI-DJ_FA",
        "outputId": "6b0d6527-01f1-4969-d2f2-c4abeb99829c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----BEGIN OPENSSH PRIVATE KEY-----\n",
            "b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\n",
            "QyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQAAAAJhAtW8YQLVv\n",
            "GAAAAAtzc2gtZWQyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQA\n",
            "AAAEA6ARNr020VevD7mkC4GFBVqlTcZP7hvn8B3xi5LDvzYIHdyU5p+Lp9loGBAI3b1NXI\n",
            "ko1b95VFdPj8HgwHbxVAAAAAEHNpbW9uZUBhcmNobGludXgBAgMEBQ==\n",
            "-----END OPENSSH PRIVATE KEY-----\n",
            "id_ecdsa  known_hosts\n",
            "Cloning into '/content/AML-project'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 24 (delta 1), reused 15 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (24/24), 57.25 KiB | 563.00 KiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ],
      "source": [
        "#@title Clone the new updated Python files from GitHub, from master\n",
        "if IN_COLAB:\n",
        "  !mkdir -p /root/.ssh\n",
        "  !touch /root/.ssh/id_ecdsa\n",
        "\n",
        "  with open('/root/.ssh/id_ecdsa', 'w') as f:\n",
        "    git_ssh_private_key = \"\"\"\n",
        "        -----BEGIN OPENSSH PRIVATE KEY-----\n",
        "        b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\n",
        "        QyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQAAAAJhAtW8YQLVv\n",
        "        GAAAAAtzc2gtZWQyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQA\n",
        "        AAAEA6ARNr020VevD7mkC4GFBVqlTcZP7hvn8B3xi5LDvzYIHdyU5p+Lp9loGBAI3b1NXI\n",
        "        ko1b95VFdPj8HgwHbxVAAAAAEHNpbW9uZUBhcmNobGludXgBAgMEBQ==\n",
        "        -----END OPENSSH PRIVATE KEY-----\n",
        "    \"\"\"\n",
        "    f.write('\\n'.join([line.strip() for line in git_ssh_private_key.split('\\n') if line.strip() ]) + '\\n')\n",
        "\n",
        "  with open('/root/.ssh/known_hosts', 'w') as f:\n",
        "    f.write(\"github.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\\n\")\n",
        "    f.write(\"github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\\n\")\n",
        "    f.write(\"github.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\\n\")\n",
        "\n",
        "  !chmod 400 ~/.ssh/id_ecdsa ~/.ssh/known_hosts\n",
        "  !ls ~/.ssh\n",
        "\n",
        "  # Clone the repository\n",
        "  !rm -rf /content/AML-project {DRIVE_PATH}/hallucination_detector\n",
        "  !git clone git@github.com:simonesestito/AML-project.git --depth=1 /content/AML-project\n",
        "  !mv /content/AML-project/hallucination_detector {DRIVE_PATH}\n",
        "  assert os.path.exists('/content/AML-project/.git'), 'Error cloning the repository. See logs above for details'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly5uv1XmJ_FB"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 1\n",
        "%aimport hallucination_detector\n",
        "import hallucination_detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMraWvbuJ_FB"
      },
      "source": [
        "# Initialize Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiCsNDlgJ_FC"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import lightning as pl\n",
        "from hallucination_detector.llama import LlamaInstruct\n",
        "from hallucination_detector.dataset import StatementDataModule\n",
        "from hallucination_detector.extractor import LlamaHiddenStatesExtractor\n",
        "from hallucination_detector.classifier import OriginalSAPLMAClassifier\n",
        "\n",
        "torch.set_default_dtype(torch.float16)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEjB3pgZJ_FC"
      },
      "outputs": [],
      "source": [
        "llama = LlamaInstruct()\n",
        "assert not IN_COLAB or llama.device.type == 'cuda', 'The model should be running on a GPU. On CPU, it is impossible to run'\n",
        "\n",
        "if llama.device.type == 'cpu':\n",
        "    print('WARNING: You are running an LLM on the CPU. Beware of the long inference times! Use it ONLY FOR SMALL tests, like very small tests.', file=sys.stderr, flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3nUefVqJ_FC"
      },
      "source": [
        "# Implement SAPLMA original model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcdMKoHwJ_FC"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "datamodule = StatementDataModule(batch_size=batch_size, drive_path='publicDataset')\n",
        "datamodule.prepare_data()\n",
        "print(f'Found {len(datamodule.full_dataset)} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olBUVyGrJ_FD"
      },
      "outputs": [],
      "source": [
        "original_saplma = OriginalSAPLMAClassifier()\n",
        "original_saplma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UzIhzWTJ_FD"
      },
      "source": [
        "## Train the SAPLMA classifier\n",
        "\n",
        "According to the original paper, including the hidden layer index, and so on.\n",
        "\n",
        "The only difference is that we are using llama **3.2** instead of version 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMnKe35CJ_FD"
      },
      "outputs": [],
      "source": [
        "# topics\n",
        "topics = datamodule.full_dataset.get_topics()\n",
        "print(f'Found {len(topics)} topics')\n",
        "topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0COPRmp5J_FD"
      },
      "outputs": [],
      "source": [
        "class OriginalSAPLMAClassifierE2E(pl.LightningModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      llama: LlamaInstruct,\n",
        "      saplma_classifier: nn.Module,\n",
        "      hidden_states_layer_idx: int,\n",
        "      lr: float = 1e-5,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    llama.eval()\n",
        "    self.hidden_states_extractor = LlamaHiddenStatesExtractor(llama)\n",
        "    self.saplma_classifier = saplma_classifier\n",
        "\n",
        "    ######### REMOVE THIS ############\n",
        "    test_states = self.hidden_states_extractor.extract_input_hidden_states_for_layer(prompt=['hello', 'goodbye'], for_layer=13)\n",
        "    print('test_states', test_states.shape)\n",
        "\n",
        "    self.hidden_states_layer_idx = hidden_states_layer_idx\n",
        "    self.lr = lr\n",
        "    self.save_hyperparameters('hidden_states_layer_idx', 'lr')\n",
        "\n",
        "\n",
        "  def forward(self, statements: tuple[str], labels: torch.Tensor):\n",
        "    # Extract statements hidden states\n",
        "    hidden_states = self.hidden_states_extractor.extract_input_hidden_states_for_layers(prompt=statements, for_layers=[self.hidden_states_layer_idx])\n",
        "\n",
        "    # Average across all the input tokens\n",
        "    print('hidden_states', hidden_states.shape)\n",
        "    avg_hidden_states = torch.mean(hidden_states, dim=1)\n",
        "    print('avg_hidden_states', avg_hidden_states.shape)\n",
        "\n",
        "    # Classify\n",
        "    return self.saplma_classifier(avg_hidden_states)\n",
        "\n",
        "\n",
        "  def do_step(self, batch, prefix_str: str):\n",
        "    statements, labels, _ = batch\n",
        "    assert isinstance(statements, tuple), f'Expected statements to be a tuple. Found: {type(statements)}'\n",
        "    assert isinstance(labels, torch.Tensor), f'Expected labels to be a tensor. Found: {type(labels)}'\n",
        "    assert len(labels.shape) == 1, f'Expected labels to be a 1D tensor. Found: {labels.shape}'\n",
        "    assert labels.size(0) == len(statements), f'Expected labels to have the same size as statements. Found: {labels.size(0)} != {len(statements)}'\n",
        "\n",
        "    preds = self.forward(statements, labels)\n",
        "\n",
        "    loss = F.cross_entropy(preds, labels)\n",
        "    self.log(f'{prefix_str}_loss', loss, prog_bar=True)\n",
        "\n",
        "    acc = (preds.detach().argmax(dim=-1) == labels).float().mean()\n",
        "    self.log(f'{prefix_str}_acc', acc, prog_bar=True)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    return self.do_step(batch, 'train')\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    return self.do_step(batch, 'val')\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.AdamW(self.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcI-MVloJ_FD"
      },
      "outputs": [],
      "source": [
        "model = OriginalSAPLMAClassifierE2E(llama, original_saplma, hidden_states_layer_idx=13, lr=1e-5)\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10)\n",
        "datamodule.set_test_topic('cities_true_false')\n",
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}