{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Replicate SAPLMA\n",
        "In this notebook, we aim to replicate what the original paper has done, in order to have a working baseline.\n",
        "\n",
        "Source: https://arxiv.org/pdf/2304.13734"
      ],
      "metadata": {
        "id": "WvYDGgUYiiKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports, installations and declarations from previous notebooks\n",
        "\n",
        "This section can be skipped and collapsed."
      ],
      "metadata": {
        "id": "uXknG8gh2l6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Drive, if needed, and check the HF_TOKEN is set and accessible\n",
        "import os\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH: str = '/content/drive/MyDrive/Final_Project/'\n",
        "assert os.path.exists(DRIVE_PATH), 'Did you forget to create a shortcut in MyDrive named Final_Project this time as well? :('\n",
        "%cd {DRIVE_PATH}\n",
        "!ls\n",
        "\n",
        "print()\n",
        "assert userdata.get('HF_TOKEN'), 'Set up HuggingFace login secret properly in Colab!'\n",
        "print('HF_TOKEN found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJq0xkuVyLFx",
        "outputId": "9e602fbb-9be3-4f78-e575-f6e5afec6a60",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/.shortcut-targets-by-id/1WdIP20OinXKeEN_xVOHEa6WVcY4eSO-k/Final_Project\n",
            " 1_experiments_on_llama_and_saplma.ipynb   hallucination_detector\n",
            " 2_replicate_saplma.ipynb\t\t   publicDataset\n",
            "'AML - First presentation.gslides'\t   X_create_saplma_tensors_dataset.ipynb\n",
            "\n",
            "HF_TOKEN found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 1\n",
        "%aimport hallucination_detector\n",
        "import hallucination_detector"
      ],
      "metadata": {
        "id": "r3aWt5Cx9ddW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Llama"
      ],
      "metadata": {
        "id": "VH6mBr9c63jm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1IOdwRgjkQvO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from hallucination_detector.llama import LlamaInstruct\n",
        "from hallucination_detector.dataset import StatementDataset\n",
        "from hallucination_detector.extractor import LlamaHiddenStatesExtractor\n",
        "from hallucination_detector.classifier import OriginalSAPLMAClassifier\n",
        "\n",
        "torch.set_default_dtype(torch.float16)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lGmDcnZQMpBe"
      },
      "outputs": [],
      "source": [
        "llama = LlamaInstruct()\n",
        "assert llama.device.type == 'cuda', 'The model should be running on a GPU. On CPU, it is impossible to run'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement SAPLMA original model"
      ],
      "metadata": {
        "id": "JhOnYO3D6-tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = StatementDataset.load_from_directory('publicDataset')\n",
        "print(f'Found {len(dataset)} samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i1eT6hA91EG",
        "outputId": "a0506edb-36be-4182-cd26-2a7ed1f354f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading file: cities_true_false.csv\n",
            "Loading file: animals_true_false.csv\n",
            "Loading file: elements_true_false.csv\n",
            "Loading file: inventions_true_false.csv\n",
            "Loading file: companies_true_false.csv\n",
            "Loading file: generated_true_false.csv\n",
            "Loading file: facts_true_false.csv\n",
            "Found 6330 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_saplma = OriginalSAPLMAClassifier()\n",
        "original_saplma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgrxmaaY-piE",
        "outputId": "1624c8f7-e396-4104-d465-219cef8ac9e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OriginalSAPLMAClassifier(\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_extractor = LlamaHiddenStatesExtractor(llama)\n",
        "hidden_extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nMydoVr--Pw",
        "outputId": "843d02ef-f18d-44fd-c991-c340a44c3c44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<hallucination_detector.extractor.hidden_states.LlamaHiddenStatesExtractor at 0x7d89f8e1edd0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the SAPLMA classifier\n",
        "\n",
        "According to the original paper, including the hidden layer index, and so on.\n",
        "\n",
        "The only difference is that we are using llama **3.2** instead of version 2."
      ],
      "metadata": {
        "id": "LrHaGygeACBp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pAe2lw2bAWko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}