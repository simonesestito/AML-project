{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Replicate SAPLMA\n",
        "In this notebook, we aim to replicate what the original paper has done, in order to have a working baseline.\n",
        "\n",
        "Source: https://arxiv.org/pdf/2304.13734"
      ],
      "metadata": {
        "id": "WvYDGgUYiiKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports, installations and declarations from previous notebooks\n",
        "\n",
        "This section can be skipped and collapsed."
      ],
      "metadata": {
        "id": "uXknG8gh2l6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Drive, if needed, and check the HF_TOKEN is set and accessible\n",
        "import os\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH: str = '/content/drive/MyDrive/Final_Project/'\n",
        "assert os.path.exists(DRIVE_PATH), 'Did you forget to create a shortcut in MyDrive named Final_Project this time as well? :('\n",
        "%cd {DRIVE_PATH}\n",
        "!ls\n",
        "\n",
        "print()\n",
        "assert userdata.get('HF_TOKEN'), 'Set up HuggingFace login secret properly in Colab!'\n",
        "print('HF_TOKEN found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJq0xkuVyLFx",
        "outputId": "330004c9-2845-4d29-8b2b-5455acd1e76b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1WdIP20OinXKeEN_xVOHEa6WVcY4eSO-k/Final_Project\n",
            " 1_experiments_on_llama_and_saplma.ipynb   publicDataset\n",
            " 2_replicate_saplma.ipynb\t\t   X_create_saplma_tensors_dataset.ipynb\n",
            "'AML - First presentation.gslides'\n",
            "\n",
            "HF_TOKEN found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone the new updated Python files from GitHub, from master\n",
        "!mkdir -p /root/.ssh\n",
        "!touch /root/.ssh/id_ecdsa\n",
        "\n",
        "with open('/root/.ssh/id_ecdsa', 'w') as f:\n",
        "  f.write(\"\"\"-----BEGIN OPENSSH PRIVATE KEY-----\n",
        "b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\n",
        "QyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQAAAAJhAtW8YQLVv\n",
        "GAAAAAtzc2gtZWQyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQA\n",
        "AAAEA6ARNr020VevD7mkC4GFBVqlTcZP7hvn8B3xi5LDvzYIHdyU5p+Lp9loGBAI3b1NXI\n",
        "ko1b95VFdPj8HgwHbxVAAAAAEHNpbW9uZUBhcmNobGludXgBAgMEBQ==\n",
        "-----END OPENSSH PRIVATE KEY-----\\n\"\"\")\n",
        "\n",
        "with open('/root/.ssh/known_hosts', 'w') as f:\n",
        "  f.write(\"github.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\\n\")\n",
        "  f.write(\"github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\\n\")\n",
        "  f.write(\"github.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\\n\")\n",
        "\n",
        "!chmod 400 ~/.ssh/id_ecdsa ~/.ssh/known_hosts\n",
        "!ls ~/.ssh\n",
        "\n",
        "# Clone the repository\n",
        "!rm -rf /content/AML-project {DRIVE_PATH}/hallucination_detector\n",
        "!git clone git@github.com:simonesestito/AML-project.git --depth=1 /content/AML-project\n",
        "!mv /content/AML-project/hallucination_detector {DRIVE_PATH}\n",
        "assert os.path.exists('/content/AML-project/.git'), 'Error cloning the repository. See logs above for details'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhcEU8LsEQ0x",
        "outputId": "ad0623df-7c18-44dc-9df5-8cb9918edaed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_ecdsa  known_hosts\n",
            "Cloning into '/content/AML-project'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 23 (delta 1), reused 16 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (23/23), 55.06 KiB | 517.00 KiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 1\n",
        "%aimport hallucination_detector\n",
        "import hallucination_detector"
      ],
      "metadata": {
        "id": "r3aWt5Cx9ddW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Llama"
      ],
      "metadata": {
        "id": "VH6mBr9c63jm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IOdwRgjkQvO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from hallucination_detector.llama import LlamaInstruct\n",
        "from hallucination_detector.dataset import StatementDataset\n",
        "from hallucination_detector.extractor import LlamaHiddenStatesExtractor\n",
        "from hallucination_detector.classifier import OriginalSAPLMAClassifier\n",
        "\n",
        "torch.set_default_dtype(torch.float16)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGmDcnZQMpBe"
      },
      "outputs": [],
      "source": [
        "llama = LlamaInstruct()\n",
        "assert llama.device.type == 'cuda', 'The model should be running on a GPU. On CPU, it is impossible to run'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement SAPLMA original model"
      ],
      "metadata": {
        "id": "JhOnYO3D6-tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = StatementDataset.load_from_directory('publicDataset')\n",
        "print(f'Found {len(dataset)} samples')"
      ],
      "metadata": {
        "id": "_i1eT6hA91EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_saplma = OriginalSAPLMAClassifier()\n",
        "original_saplma"
      ],
      "metadata": {
        "id": "dgrxmaaY-piE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_extractor = LlamaHiddenStatesExtractor(llama)\n",
        "hidden_extractor"
      ],
      "metadata": {
        "id": "_nMydoVr--Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the SAPLMA classifier\n",
        "\n",
        "According to the original paper, including the hidden layer index, and so on.\n",
        "\n",
        "The only difference is that we are using llama **3.2** instead of version 2."
      ],
      "metadata": {
        "id": "LrHaGygeACBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# topics\n",
        "topics = dataset.get_topics()\n",
        "print(f'Found {len(topics)} topics')"
      ],
      "metadata": {
        "id": "pAe2lw2bAWko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}