# Study on Hallucination Detection by LLMs hidden state analysis

As a task for our final project for the *Advanced Machine Learning 2024/2025* course at Sapienza, we chose **hallucination detection**, which consists in predicting when a Large Language Model is generating false or misleading information, even though it would have the needed knowledge to answer correctly. 

![Related works](https://github.com/simonesestito/AML-project/blob/master/images/Related_Works.png)

## SAPLMA
In particular, we focus on the work in **The Internal State of an LLM Knows When It’s Lying** [*Azaria and Mitchell, 2023*], where the model *SAPLMA* (Statement Accuracy Prediction based on LLM Activations) is presented. It is a simple classifier which takes as input the hidden states produced at the intermediate layers of a reference LLM when processing or generating a token sequence. In fact, the authors aim to test if the LLM inherently contains information about the truthfulness of the statements it observes in its internal activations. We replicated their system and made some more experiments to confirm such intuition. We also leveraged the *True/False Statements dataset* they created, comprising 6,084 sentences from 6 different topics (scientific facts, chemical elements, cities, animals, companies and inventions) paired with a binary truth label. In addition, 245 statements generated by the OPT 6.7b LLM are included.

## Uncertainty Estimation

Other works approach the same problem with a different perspective, under the name of *uncertainty estimation*. In **Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach** [*Liu et al., 2024*] the authors formalize the task as learning a function $g$ that could predict the similarity or coherence between an LLM output $y$ and a correct output $y_{true}$ to an input prompt $x$, to measure the LLM uncertainty in a supervised question answering setting. They leverage both the LLM hidden states, as in *[Azaria and Mitchell, 2023]*, and other probability or entropy related features, based on the distribution generated by the LLM on the corresponding vocabulary. This way, they can address the problem of uncertainty estimation also on "grey-box"  LLMs, where access to internals such as hidden states could be not allowed. 

However, the authors stress the importance of leveraging the LLMs hidden states and try to give a formal proof of the fact that they contain information that is not already encoded in the LLM output logits.

## Analysis of LLM hidden states

A further investigation into the significance of LLM hidden states for hallucination detection can be found in **Do LLMs Know about Hallucination? An Empirical Investigation of LLM’s Hidden States** [*Duan et al., 2024*], where the author conduct multiple experiments to study the difference in the hidden states generated from hallucinated and correct statements. In particular, they analysed the transitions vectors that connect the hidden states of an input prompt to those of a correct and an hallucinated response, and individuated two different directions ($d_{corr}$ for correct responses, $d_{halluc}$ for hallucinated ones) that, when projected onto the vocabulary space, corresponds to tokens semantically related to truthfulness of statements (such as '_Correct' or '_Right' for $d_{corr}$ and 'False' or '_Sad' for $d_{halluc}$).

## Supervised and unsupervised approaches

The task of hallucination detection can be addressed in different ways. We followed the supervised approach of  [*Azaria and Mitchell, 2023*], but there were other options.

We could have followed a more traditional approach such as that in **Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models** [*Duan et al., 2024*], where LLM uncertainty is predicted by computing the entropy of the generated tokens. The authors suggest that weighting the tokens by their semantic relevance in a sentence, and/or considering the sentence-level importance in the LLM response, could improve the estimation performance.

Or we could have chosen an unsupervised setting to hallucination detection leveraging LLM's hidden states, as in **INSIDE: LLM's Internal State Retains the Power of Hallucination Detection** [*Chen et al., 2024*]. The authors discriminate between correct and hallucinated LLM responses by computing the semantic divergence of multiple responses generated from the same prompt in the hidden sentence embedding space. If the eigenvectors of the covariance matrix of the sentences embeddings are significantly larger than 0, the LLM is likely less confident about its answers and more prone to hallucination.

## Queries vs. Probes

The work on SAPLMA by [*Azaria and Mitchell, 2023*] is contextualized in **Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?** [*Liu et al., 2023*], where the authors explore the differences in the LLM queries output and their internal activations (*probes*), such as the hidden states. They categorize the disagreements between queries and probes as *Model Confabulation* (when probe confidence is low and the completions from queries are confidently incorrect), *Heterogeneity* (when probes and queries exhibit different accuracy on specific input subsets) and *Deception* (when the probe is confidently correct and the query completion is confidently incorrect), where the work on SAPLMA by [*Azaria and Mitchell, 2023*] is collocated. In fact, the authors show that most of the times probes outperform queries, but also that we could benefit from training an ensemble model leveraging both LLM probability distribution and hidden state features.



### References
- *The Internal State of an LLM Knows When It’s Lying*, Azaria and Mitchell, 2023 (https://arxiv.org/pdf/2304.13734)
-  *INSIDE: LLM's Internal State Retains the Power of Hallucination Detection*, Chen et al., 2024 (https://arxiv.org/pdf/2402.03744)
- *Do LLMs Know about Hallucination? An Empirical Investigation of LLM’s Hidden States*, Duan et al., 2024 (https://arxiv.org/pdf/2402.09733)
- *Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models*, Duan et al., 2024 (https://arxiv.org/pdf/2307.01379)
- *Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?*, Liu et al., 2023 (https://arxiv.org/pdf/2312.03729)
- *Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach*, Liu et al., 2024 (https://arxiv.org/pdf/2404.15993)


