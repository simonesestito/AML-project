{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Per token inference\n",
        "\n",
        "Now that we have a model that works well with the last token,\n",
        "we want to explore how it behaves when used in a real-time environment,\n",
        "so that we may eventually have an hallucination probability inferred **while generating**, per single token."
      ],
      "metadata": {
        "id": "d62s8Z0tw39D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7riZzynJ_E9"
      },
      "source": [
        "# Imports, installations and declarations from previous notebooks\n",
        "\n",
        "This section can be skipped and collapsed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JXs-2fCJ_E-",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48144de6-836c-43ba-e00f-1625e3907925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.10.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.9)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.0)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.6)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.9)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "#@title Install missing dependencies\n",
        "!pip install wandb lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U1bN2Y-J_E_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH7N5rw6J_FA"
      },
      "outputs": [],
      "source": [
        "# If not in Colab, do some compatibility changes\n",
        "if not IN_COLAB:\n",
        "    DRIVE_PATH='.'\n",
        "    os.environ['HF_TOKEN'] = open('.hf_token').read().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fP6gb_MJ_FA",
        "outputId": "bc84e173-a7ec-44a9-9456-f642949d5db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content\n",
            "artifacts  drive  hallucination_detector  publicDataset  sample_data  wandb\n",
            "\n",
            "HF_TOKEN found\n",
            "WANDB_API_KEY found and set as env var\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Drive, if needed, and check the HF_TOKEN is set and accessible\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive, userdata\n",
        "\n",
        "    drive.mount('/content/drive', readonly=True)\n",
        "    DRIVE_PATH: str = '/content/drive/MyDrive/Final_Project/'\n",
        "    assert os.path.exists(DRIVE_PATH), 'Did you forget to create a shortcut in MyDrive named Final_Project this time as well? :('\n",
        "    !cp -R {DRIVE_PATH}/publicDataset .\n",
        "    !pwd\n",
        "    !ls\n",
        "    print()\n",
        "\n",
        "    assert userdata.get('HF_TOKEN'), 'Set up HuggingFace login secret properly in Colab!'\n",
        "    print('HF_TOKEN found')\n",
        "\n",
        "    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "    print('WANDB_API_KEY found and set as env var')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYKSjI-DJ_FA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f898da-1973-4336-8eb0-c735d3d9fb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_ecdsa  known_hosts\n",
            "Cloning into '/content/AML-project'...\n",
            "remote: Enumerating objects: 424, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 424 (delta 45), reused 56 (delta 30), pack-reused 338 (from 1)\u001b[K\n",
            "Receiving objects: 100% (424/424), 2.14 MiB | 1.57 MiB/s, done.\n",
            "Resolving deltas: 100% (230/230), done.\n"
          ]
        }
      ],
      "source": [
        "#@title Clone the new updated Python files from GitHub, from master\n",
        "if IN_COLAB:\n",
        "  !mkdir -p /root/.ssh\n",
        "  !touch /root/.ssh/id_ecdsa\n",
        "\n",
        "  with open('/root/.ssh/id_ecdsa', 'w') as f:\n",
        "    git_ssh_private_key = \"\"\"\n",
        "        -----BEGIN OPENSSH PRIVATE KEY-----\n",
        "        b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\n",
        "        QyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQAAAAJhAtW8YQLVv\n",
        "        GAAAAAtzc2gtZWQyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQA\n",
        "        AAAEA6ARNr020VevD7mkC4GFBVqlTcZP7hvn8B3xi5LDvzYIHdyU5p+Lp9loGBAI3b1NXI\n",
        "        ko1b95VFdPj8HgwHbxVAAAAAEHNpbW9uZUBhcmNobGludXgBAgMEBQ==\n",
        "        -----END OPENSSH PRIVATE KEY-----\n",
        "    \"\"\"\n",
        "    f.write('\\n'.join([line.strip() for line in git_ssh_private_key.split('\\n') if line.strip() ]) + '\\n')\n",
        "\n",
        "  with open('/root/.ssh/known_hosts', 'w') as f:\n",
        "    f.write(\"github.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\\n\")\n",
        "    f.write(\"github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\\n\")\n",
        "    f.write(\"github.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\\n\")\n",
        "\n",
        "  !chmod 400 ~/.ssh/id_ecdsa ~/.ssh/known_hosts\n",
        "  !ls ~/.ssh\n",
        "\n",
        "  # Clone the repository\n",
        "  !rm -rf /content/AML-project\n",
        "  !git clone git@github.com:simonesestito/AML-project.git /content/AML-project\n",
        "  assert os.path.exists('/content/AML-project/.git'), 'Error cloning the repository. See logs above for details'\n",
        "  !rm -rf ./hallucination_detector && mv /content/AML-project/hallucination_detector .\n",
        "  !rm -rf /content/AML-project  # We don't need the Git repo anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly5uv1XmJ_FB"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 1\n",
        "%aimport hallucination_detector\n",
        "import hallucination_detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMraWvbuJ_FB"
      },
      "source": [
        "# Initialize Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiCsNDlgJ_FC"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import lightning as pl\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from hallucination_detector.llama import LlamaInstruct, LlamaPrompt\n",
        "from hallucination_detector.dataset import StatementDataModule\n",
        "from hallucination_detector.extractor import LlamaHiddenStatesExtractor, WeightedMeanReduction, AttentionAwareWeightedMeanReduction\n",
        "from hallucination_detector.classifier import OriginalSAPLMAClassifier, LightningHiddenStateSAPLMA, EnhancedSAPLMAClassifier\n",
        "from hallucination_detector.utils import try_to_overfit, plot_weight_matrix, classificator_evaluation\n",
        "import wandb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEjB3pgZJ_FC"
      },
      "outputs": [],
      "source": [
        "llama = LlamaInstruct()\n",
        "assert not IN_COLAB or llama.device.type == 'cuda', 'The model should be running on a GPU. On CPU, it is impossible to run'\n",
        "\n",
        "if llama.device.type == 'cpu':\n",
        "    print('WARNING: You are running an LLM on the CPU. Beware of the long inference times! Use it ONLY FOR SMALL tests, like very small tests.', file=sys.stderr, flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize trained `SAPLMAClassifier`"
      ],
      "metadata": {
        "id": "P4QtrsCbxV8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saplma_artifact_id = 'aml-2324-project/llama-hallucination-detector/attention-aware-weighted-tokens-architecture-hc7ivucr:best'\n",
        "\n",
        "run = wandb.init()\n",
        "artifact = run.use_artifact(saplma_artifact_id, type='model')\n",
        "artifact_dir = artifact.download()\n",
        "artifact_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "oYI9CJK6xgf8",
        "outputId": "db702834-c003-4c9a-c272-a3da550b7772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msestito-1937764\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241209_144520-k30b1mnq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sestito-1937764/uncategorized/runs/k30b1mnq' target=\"_blank\">toasty-deluge-5</a></strong> to <a href='https://wandb.ai/sestito-1937764/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sestito-1937764/uncategorized' target=\"_blank\">https://wandb.ai/sestito-1937764/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sestito-1937764/uncategorized/runs/k30b1mnq' target=\"_blank\">https://wandb.ai/sestito-1937764/uncategorized/runs/k30b1mnq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/artifacts/attention-aware-weighted-tokens-architecture-hc7ivucr:v10'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {artifact_dir}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBrWUzSlxwV8",
        "outputId": "1aeaacbc-d0e7-4609-8b55-64ea83c76d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saplma = LightningHiddenStateSAPLMA.load_from_checkpoint(\n",
        "    os.path.join(artifact_dir, 'model.ckpt'),\n",
        "    llama=llama,\n",
        "    saplma_classifier=OriginalSAPLMAClassifier(),\n",
        "    reduction=AttentionAwareWeightedMeanReduction(),\n",
        ").eval()"
      ],
      "metadata": {
        "id": "BZGI4RK2zg3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "_mHDKiLs0dIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hallucination_detector.extractor.tokenizer import tokenize_prompts_fixed_length"
      ],
      "metadata": {
        "id": "REHp5NYy1Cr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "datamodule = StatementDataModule(batch_size=batch_size, drive_path='publicDataset')\n",
        "datamodule.prepare_data()\n",
        "print(f'Found {len(datamodule.full_dataset)} samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZt8dFmS0ce3",
        "outputId": "e544e588-5c7f-4f7f-bc66-8479b87c8ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading file: cities_true_false.csv\n",
            "Loading file: facts_true_false.csv\n",
            "Loading file: animals_true_false.csv\n",
            "Loading file: elements_true_false.csv\n",
            "Loading file: inventions_true_false.csv\n",
            "Loading file: companies_true_false.csv\n",
            "Loading file: generated_true_false.csv\n",
            "Found 6330 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample = datamodule.full_dataset[980]\n",
        "random_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PSlL-VF0a0h",
        "outputId": "6bf8a65d-beb6-4068-db41-c2598eed0bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Tokyo is a name of a country.', tensor(0), 'cities_true_false')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_prefix_suffix_from_tokens(tokens: torch.Tensor) -> torch.Tensor:\n",
        "  '''\n",
        "  Given a tensor output from some test function,\n",
        "  remove the prefix and suffix tokens included in the prompt,\n",
        "  leaving us with only the user input.\n",
        "  '''\n",
        "\n",
        "  random_user_input = 'y6BabNgCyZf3A9XC3d1Qr'\n",
        "\n",
        "  # Extract the strings for prefix and suffix, that are added by LlamaPrompt\n",
        "  full_llama_prompt = str(LlamaPrompt(random_user_input))\n",
        "  prefix_len = full_llama_prompt.index(random_user_input)\n",
        "  suffix_len = len(full_llama_prompt) - prefix_len - len(random_user_input)\n",
        "  prefix, suffix = full_llama_prompt[:prefix_len], full_llama_prompt[-suffix_len:]\n",
        "\n",
        "  # Count how many tokens do they require\n",
        "  prefix_len = llama.tokenizer([prefix], return_tensors='pt').input_ids.ravel().size(0)\n",
        "  suffix_len = llama.tokenizer([suffix], return_tensors='pt').input_ids.ravel().size(0)\n",
        "\n",
        "  # Remove the tokens that are not part of the user input we want to analyze\n",
        "  tokens = tokens[prefix_len:-suffix_len+1]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "QCPs6e2_N285"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test_single_tokens_with_saplma_inference(statement: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    '''\n",
        "    Run SALPMA on the single tokens of a statement.\n",
        "    Thus, inferring the hallucination probability of a single token,\n",
        "    maybe while generating... in theory\n",
        "    '''\n",
        "    tokenized_sample = tokenize_prompts_fixed_length(llama, statement)\n",
        "    token_ids, attn_mask = tokenized_sample.input_ids.squeeze(), tokenized_sample.attention_mask.squeeze()\n",
        "    real_token_ids = token_ids[attn_mask == 1]\n",
        "\n",
        "    model_dtype = next(saplma.saplma_classifier.parameters()).dtype\n",
        "\n",
        "    hidden_states = saplma.hidden_states_extractor.extract_input_hidden_states_for_layer(\n",
        "        prompt=statement,\n",
        "        for_layer=11,\n",
        "    ).detach().to(model_dtype)[0]   # returns [70, 2048] = [TOKENS, EMBEDDING_DIM]\n",
        "    assert hidden_states.shape == (70, 2048)\n",
        "\n",
        "    # Consider each token as a sample in a batch\n",
        "    # ignoring the ones that are not part of the real input statement\n",
        "    real_hidden_states = hidden_states[attn_mask == 1]\n",
        "    assert len(real_hidden_states.shape) == 2\n",
        "    assert real_hidden_states.size(1) == 2048\n",
        "\n",
        "    ## (!!!) For the hallucination classification of each token,\n",
        "    #        take the opposite result.\n",
        "    #        This is simply because the model outputs 1 for True sentences\n",
        "    #        and 0 for False/Hallucinated ones.\n",
        "    #        We want an hallucination score, so we must take 1 - classifier_output\n",
        "    each_token_classification = 1 - saplma.saplma_classifier(real_hidden_states)\n",
        "\n",
        "    # Remove the tokens that are not part of the user input we want to analyze\n",
        "    each_token_classification = remove_prefix_suffix_from_tokens(each_token_classification)\n",
        "    real_token_ids = remove_prefix_suffix_from_tokens(real_token_ids)\n",
        "\n",
        "    return each_token_classification, real_token_ids\n",
        "\n",
        "\n",
        "print(random_sample[0])\n",
        "each_token_classification, real_token_ids = test_single_tokens_with_saplma_inference(random_sample[0])\n",
        "\n",
        "for hallucination_probability, token in zip(each_token_classification, real_token_ids):\n",
        "    print(f'{hallucination_probability.item():>6.1%}: {llama.tokenizer.decode(token)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T73EpsT612ed",
        "outputId": "8c3728b8-a44c-4b7f-cd5d-6a4c91e2fa9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokyo is a name of a country.\n",
            " 92.8%: Tok\n",
            " 99.0%: yo\n",
            " 94.5%:  is\n",
            "100.0%:  a\n",
            " 95.7%:  name\n",
            " 99.8%:  of\n",
            "100.0%:  a\n",
            "100.0%:  country\n",
            "100.0%: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try with gradients\n",
        "\n",
        "A similar approach to gradcam"
      ],
      "metadata": {
        "id": "MgGjqhg0-Wv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Copy of the same function in the Python files,\n",
        "# but now it also returns embeddings of the input,\n",
        "# with requires_grad=True and retain_grad()\n",
        "#\n",
        "def extract_input_hidden_states_for_layers(prompt, for_layers: set[int]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Given a batch of prompts, with length BATCH_SIZE,\n",
        "    extract the hidden states for the L requested layers.\n",
        "\n",
        "    The output tensor will have shape [BATCH_SIZE, L, SEQ_LEN, TOKEN_DIM].\n",
        "    SEQ_LEN is the length of the input sequence, which is the same for all prompts in the batch, fixed to 70.\n",
        "    TOKEN_DIM is the dimension of the hidden states, which is the same for all layers in the model, fixed to 2048.\n",
        "    \"\"\"\n",
        "    if isinstance(for_layers, list) or isinstance(for_layers, tuple):\n",
        "        for_layers = set(for_layers)\n",
        "    assert isinstance(for_layers, set), f\"Expected for_layers to be a set. Found: {type(for_layers)}\"\n",
        "\n",
        "    max_layers = len(llama.iter_layers())\n",
        "    assert all(0 <= layer < max_layers for layer in for_layers), f\"Expected all layers to be in range [0, {max_layers}). Found: {for_layers}\"\n",
        "\n",
        "    hidden_states = []\n",
        "\n",
        "    def _collect_hidden_states(layer_idx: int):\n",
        "        def _hook(module, inputs, outputs):\n",
        "            assert isinstance(outputs, tuple), f\"Expected outputs to be a tuple. Found: {type(outputs)}\"\n",
        "            assert len(outputs) >= 1, f\"Expected outputs to have 1+ elements. Found: {len(outputs)}\"\n",
        "\n",
        "            hidden_state = outputs[0]\n",
        "            assert isinstance(hidden_state, torch.Tensor), f\"Expected hidden_state to be a torch.Tensor. Found: {type(hidden_state)}\"\n",
        "            assert hidden_state.size(1) == 70 and hidden_state.size(2) == 2048, f\"Expected hidden_state to have shape (?, 70, 2048). Found: {hidden_state.shape}\"\n",
        "            hidden_states.append(hidden_state)\n",
        "        return _hook\n",
        "\n",
        "    llama.unregister_all_hooks()\n",
        "    for layer_idx, decoder_layer in enumerate(llama.iter_layers()):\n",
        "        if layer_idx in for_layers:\n",
        "            llama.register_hook(decoder_layer, _collect_hidden_states(layer_idx))\n",
        "\n",
        "    inputs = tokenize_prompts_fixed_length(llama, prompt)\n",
        "    embedded_inputs = llama.model.get_input_embeddings()(inputs.input_ids)\n",
        "    embedded_inputs = embedded_inputs.clone().detach().requires_grad_(True)\n",
        "    embedded_inputs.retain_grad()\n",
        "    _ = llama.model(\n",
        "        inputs_embeds=embedded_inputs,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        **{\n",
        "            \"max_length\": None,\n",
        "            \"max_new_tokens\": 1,\n",
        "            \"num_return_sequences\": 1,\n",
        "            # We are collecting hidden_states in a more fine-grained way with hooks\n",
        "            \"output_attentions\": False,\n",
        "            \"output_hidden_states\": False,\n",
        "            \"return_dict_in_generate\": False,\n",
        "        }\n",
        "    )\n",
        "    llama.unregister_all_hooks()\n",
        "\n",
        "    # Now, hidden_states are a list of tensors, each tensor representing the hidden_state for a layer we requested\n",
        "    return embedded_inputs, torch.stack(hidden_states).transpose(0, 1)"
      ],
      "metadata": {
        "id": "Osk29A89EPUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tokens_with_grad(statement: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    '''\n",
        "    Try to understand which tokens are the responsible for the hallucination verdict of a sentence,\n",
        "    based on the gradients they receive = their importance in the final outcome.\n",
        "    '''\n",
        "    tokenized_sample = tokenize_prompts_fixed_length(llama, statement)\n",
        "    token_ids, attn_mask = tokenized_sample.input_ids.squeeze(), tokenized_sample.attention_mask.squeeze()\n",
        "    real_token_ids = token_ids[attn_mask == 1]\n",
        "\n",
        "    # Do a forward pass, with also returning the input embeddings (with requires_grad=True)\n",
        "    embedded_inputs, hidden_states = extract_input_hidden_states_for_layers(\n",
        "        statement,\n",
        "        for_layers={11},\n",
        "    )\n",
        "    hidden_states = hidden_states.squeeze(0, 1).to(torch.float32)\n",
        "    assert hidden_states.shape == (70, 2048)\n",
        "\n",
        "    saplma_input = hidden_states[64].unsqueeze(0)\n",
        "    assert saplma_input.shape == (1, 2048)\n",
        "    prediction = saplma.saplma_classifier(saplma_input)\n",
        "    (prediction).sum().backward()  # Compute gradients on the input\n",
        "\n",
        "    # Reduce the gradients on the input embeddings, summing up all dimensions of every token\n",
        "    embedded_inputs_grads = embedded_inputs.grad[0].sum(dim=1)[attn_mask == 1]\n",
        "\n",
        "    # Remove the tokens that are not part of the user input we want to analyze\n",
        "    embedded_inputs_grads = remove_prefix_suffix_from_tokens(embedded_inputs_grads)\n",
        "    real_token_ids = remove_prefix_suffix_from_tokens(real_token_ids)\n",
        "    assert embedded_inputs_grads.shape == real_token_ids.shape\n",
        "\n",
        "    # Normalize the distribution of the gradients\n",
        "    grads_mean = torch.mean(embedded_inputs_grads)\n",
        "    grads_std = torch.std(embedded_inputs_grads)\n",
        "    embedded_inputs_grads = (embedded_inputs_grads - grads_mean) / grads_std\n",
        "\n",
        "    temperature = 0.8\n",
        "    embedded_inputs_grads = F.softmax(embedded_inputs_grads / temperature, dim=0)\n",
        "    return embedded_inputs_grads, real_token_ids\n",
        "\n",
        "\n",
        "print(random_sample[0])\n",
        "embedded_inputs_grads, real_token_ids = test_tokens_with_grad(random_sample[0])\n",
        "\n",
        "for hallucination_probability, token in zip(embedded_inputs_grads, real_token_ids):\n",
        "    print(f'{hallucination_probability.item():>6.1%}: {llama.tokenizer.decode(token)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYy6O1SV-e7O",
        "outputId": "d8bf8155-95b8-4ce7-fdb5-b657467348d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokyo is a name of a country.\n",
            " 47.3%: Tok\n",
            "  0.4%: yo\n",
            "  9.5%:  is\n",
            "  7.9%:  a\n",
            "  3.8%:  name\n",
            "  6.0%:  of\n",
            " 13.1%:  a\n",
            "  4.4%:  country\n",
            "  7.6%: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do a few more tests\n",
        "\n",
        "Compare these 2 approaches"
      ],
      "metadata": {
        "id": "yF7pBGn1OH14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tokens(sample: tuple) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    '''\n",
        "    Given a statement,\n",
        "    test it with multiple strategies and pretty print its results.\n",
        "    '''\n",
        "    statement, is_real, _ = sample\n",
        "\n",
        "    print(f'y_true: {is_real.item()} ({\"real sentence\" if is_real else \"hallucination\"}) -- y_pred: {saplma(statement).item():.2f}')\n",
        "\n",
        "    for name, strategy in (('SAPLMA infer on single tokens', test_single_tokens_with_saplma_inference),):#, ('SAPLMA gradients on embeddings', test_tokens_with_grad)):\n",
        "      probabilities, token_ids = strategy(statement)\n",
        "\n",
        "      color_thresholds = [\n",
        "          (0.65, '\\033[0m'),  # Neutral\n",
        "          (0.80, '\\033[33m'), # Yellow\n",
        "          (1.01, '\\033[31m'), # Red\n",
        "      ]\n",
        "\n",
        "      print(f'{name:>32s}:', end=' ')\n",
        "      for hallucination_probability, token in zip(probabilities, token_ids):\n",
        "          if hallucination_probability.isnan():\n",
        "              color_for_probability = '\\033[45m'  # Purple highlighted\n",
        "          else:\n",
        "            assert 0 <= hallucination_probability <= 1, f'Expected hallucination_probability to be in range [0, 1]. Found: {hallucination_probability}'\n",
        "            color_for_probability = next(style for threshold, style in color_thresholds if hallucination_probability < threshold)\n",
        "\n",
        "          print(f'{color_for_probability}{llama.tokenizer.decode(token)}\\033[0m', end='')\n",
        "      print()\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "Y0iY3u2SOJn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_true_sample = datamodule.full_dataset[1000]\n",
        "random_false_sample = datamodule.full_dataset[2000]\n",
        "\n",
        "test_tokens(random_true_sample)\n",
        "test_tokens(random_false_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpL24bZGNoIz",
        "outputId": "8b870f9c-4fa7-4e80-b9e8-638eadf93a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_true: 0 (hallucination) -- y_pred: 0.18\n",
            "   SAPLMA infer on single tokens: \u001b[33mG\u001b[0m\u001b[0mren\u001b[0m\u001b[31mada\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[0m name\u001b[0m\u001b[31m of\u001b[0m\u001b[31m a\u001b[0m\u001b[31m city\u001b[0m\u001b[31m.\u001b[0m\n",
            "\n",
            "y_true: 1 (real sentence) -- y_pred: 0.87\n",
            "   SAPLMA infer on single tokens: \u001b[31mA\u001b[0m\u001b[31m group\u001b[0m\u001b[31m of\u001b[0m\u001b[0m wolves\u001b[0m\u001b[31m is\u001b[0m\u001b[31m called\u001b[0m\u001b[31m a\u001b[0m\u001b[0m pack\u001b[0m\u001b[0m.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_true_sample = datamodule.full_dataset[1500]\n",
        "random_false_sample = datamodule.full_dataset[1701]\n",
        "\n",
        "test_tokens(random_true_sample)\n",
        "test_tokens(random_false_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRSoG0I0R-Bh",
        "outputId": "ae5e43de-7b4a-4469-ac48-aaf9c66b0c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_true: 1 (real sentence) -- y_pred: 0.99\n",
            "   SAPLMA infer on single tokens: \u001b[31mRain\u001b[0m\u001b[0mbows\u001b[0m\u001b[31m form\u001b[0m\u001b[0m when\u001b[0m\u001b[0m light\u001b[0m\u001b[0m refr\u001b[0m\u001b[0macts\u001b[0m\u001b[0m through\u001b[0m\u001b[0m water\u001b[0m\u001b[0m dro\u001b[0m\u001b[0mplets\u001b[0m\u001b[0m.\u001b[0m\n",
            "\n",
            "y_true: 0 (hallucination) -- y_pred: 0.01\n",
            "   SAPLMA infer on single tokens: \u001b[31mThe\u001b[0m\u001b[31m Earth\u001b[0m\u001b[31m's\u001b[0m\u001b[31m t\u001b[0m\u001b[0mides\u001b[0m\u001b[31m are\u001b[0m\u001b[31m primarily\u001b[0m\u001b[0m caused\u001b[0m\u001b[31m by\u001b[0m\u001b[31m the\u001b[0m\u001b[33m rep\u001b[0m\u001b[31mulsive\u001b[0m\u001b[31m push\u001b[0m\u001b[31m of\u001b[0m\u001b[31m the\u001b[0m\u001b[31m sun\u001b[0m\u001b[31m.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick 10 random FALSE samples from full_dataset\n",
        "false_sentences_indexes = [\n",
        "    sample for sample in datamodule.full_dataset if not sample[1]\n",
        "]\n",
        "random_samples = random.sample(false_sentences_indexes, 10)\n",
        "\n",
        "for sample in random_samples:\n",
        "    test_tokens(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOxiMgNaTArZ",
        "outputId": "9dfbc748-160d-42f5-8f10-c87814ee0140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_true: 0 (hallucination) -- y_pred: 0.31\n",
            "   SAPLMA infer on single tokens: \u001b[31mThe\u001b[0m\u001b[0m ham\u001b[0m\u001b[0mster\u001b[0m\u001b[33m has\u001b[0m\u001b[31m a\u001b[0m\u001b[0m habitat\u001b[0m\u001b[31m of\u001b[0m\u001b[0m coastal\u001b[0m\u001b[31m.\u001b[0m\n",
            "\n",
            "y_true: 0 (hallucination) -- y_pred: 0.02\n",
            "   SAPLMA infer on single tokens: \u001b[31mTrees\u001b[0m\u001b[31m are\u001b[0m\u001b[31m not\u001b[0m\u001b[31m plants\u001b[0m\u001b[31m,\u001b[0m\u001b[31m they\u001b[0m\u001b[31m are\u001b[0m\u001b[31m animals\u001b[0m\u001b[31m.\u001b[0m\n",
            "\n",
            "y_true: 0 (hallucination) -- y_pred: 0.18\n",
            "   SAPLMA infer on single tokens: \u001b[31mApple\u001b[0m\u001b[31m has\u001b[0m\u001b[0m headquarters\u001b[0m\u001b[0m in\u001b[0m\u001b[31m Canada\u001b[0m\u001b[31m.\u001b[0m\n",
            "\n",
            "y_true: 0 (hallucination) -- y_pred: 0.00\n",
            "   SAPLMA infer on single tokens: \u001b[31mB\u001b[0m\u001b[31mayer\u001b[0m\u001b[0mische\u001b[0m\u001b[0m Mot\u001b[0m\u001b[0moren\u001b[0m\u001b[0m Wer\u001b[0m\u001b[31mke\u001b[0m\u001b[31m (\u001b[0m\u001b[33mBMW\u001b[0m\u001b[31m)\u001b[0m\u001b[0m operates\u001b[0m\u001b[31m in\u001b[0m\u001b[31m the\u001b[0m\u001b[31m industry\u001b[0m\u001b[31m of\u001b[0m\u001b[31m Insurance\u001b[0m\u001b[31m.\u001b[0m\n",
            "\n",
            "y_true: 0 (hallucination) -- y_pred: 0.43\n",
            "   SAPLMA infer on single tokens: \u001b[31mH\u001b[0m\u001b[31mCA\u001b[0m\u001b[31m Healthcare\u001b[0m\u001b[0m operates\u001b[0m\u001b[31m as\u001b[0m\u001b[31m an\u001b[0m\u001b[31m investment\u001b[0m\u001b[0m holding\u001b[0m\u001b[0m company\u001b[0m\u001b[31m..\u001b[0m\n",
            "\n",
            "y_true: 0 (hallucination) -- y_pred: 0.37\n",
            "   SAPLMA infer on single tokens: \u001b[31mS\u001b[0m\u001b[31melenium\u001b[0m\u001b[31m is\u001b[0m\u001b[0m used\u001b[0m\u001b[31m in\u001b[0m\u001b[0m lasers\u001b[0m\u001b[31m and\u001b[0m\u001b[31m magnetic\u001b[0m\u001b[31m refriger\u001b[0m\u001b[31mation\u001b[0m\u001b[31m.\u001b[0m\n",
            "\n",
            "y_true: 0 (hallucination) -- y_pred: 0.06\n",
            "   SAPLMA infer on single tokens: \u001b[31mB\u001b[0m\u001b[0mish\u001b[0m\u001b[31mkek\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[0m name\u001b[0m\u001b[31m of\u001b[0m\u001b[31m a\u001b[0m\u001b[31m country\u001b[0m\u001b[31m.\u001b[0m\n",
            "\n",
            "y_true: 0 (hallucination) -- y_pred: 0.04\n",
            "   SAPLMA infer on single tokens: \u001b[31mCam\u001b[0m\u001b[31meroon\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[31m name\u001b[0m\u001b[31m of\u001b[0m\u001b[31m a\u001b[0m\u001b[31m city\u001b[0m\u001b[31m.\u001b[0m\n",
            "\n",
            "y_true: 0 (hallucination) -- y_pred: 0.50\n",
            "   SAPLMA infer on single tokens: \u001b[31mThe\u001b[0m\u001b[0m chimpan\u001b[0m\u001b[31mzee\u001b[0m\u001b[31m has\u001b[0m\u001b[31m a\u001b[0m\u001b[0m diet\u001b[0m\u001b[0m of\u001b[0m\u001b[0m carniv\u001b[0m\u001b[33more\u001b[0m\u001b[0m.\u001b[0m\n",
            "\n",
            "y_true: 0 (hallucination) -- y_pred: 0.56\n",
            "   SAPLMA infer on single tokens: \u001b[33mHe\u001b[0m\u001b[0min\u001b[0m\u001b[31mrich\u001b[0m\u001b[0m H\u001b[0m\u001b[0mertz\u001b[0m\u001b[0m invented\u001b[0m\u001b[31m the\u001b[0m\u001b[31m internal\u001b[0m\u001b[0m combustion\u001b[0m\u001b[0m engine\u001b[0m\u001b[33m.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick 10 random TRUE samples from full_dataset\n",
        "false_sentences_indexes = [\n",
        "    sample for sample in datamodule.full_dataset if sample[1]\n",
        "]\n",
        "random_samples = random.sample(false_sentences_indexes, 10)\n",
        "\n",
        "for sample in random_samples:\n",
        "    test_tokens(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gN4h9xl4xe3",
        "outputId": "86447d62-72d2-40d4-a66e-577d58700b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_true: 1 (real sentence) -- y_pred: 0.99\n",
            "   SAPLMA infer on single tokens: \u001b[31mThe\u001b[0m\u001b[31m scientific\u001b[0m\u001b[0m method\u001b[0m\u001b[0m is\u001b[0m\u001b[0m a\u001b[0m\u001b[0m systematic\u001b[0m\u001b[0m process\u001b[0m\u001b[0m for\u001b[0m\u001b[0m investigating\u001b[0m\u001b[0m phenomena\u001b[0m\u001b[31m and\u001b[0m\u001b[31m acquiring\u001b[0m\u001b[0m new\u001b[0m\u001b[0m knowledge\u001b[0m\u001b[0m.\u001b[0m\n",
            "\n",
            "y_true: 1 (real sentence) -- y_pred: 0.51\n",
            "   SAPLMA infer on single tokens: \u001b[31mM\u001b[0m\u001b[0mars\u001b[0m\u001b[33m has\u001b[0m\u001b[31m a\u001b[0m\u001b[0m thin\u001b[0m\u001b[0m atmosphere\u001b[0m\u001b[33m.\u001b[0m\n",
            "\n",
            "y_true: 1 (real sentence) -- y_pred: 0.46\n",
            "   SAPLMA infer on single tokens: \u001b[31mSk\u001b[0m\u001b[0mop\u001b[0m\u001b[0mje\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[31m name\u001b[0m\u001b[31m of\u001b[0m\u001b[31m a\u001b[0m\u001b[0m city\u001b[0m\u001b[33m.\u001b[0m\n",
            "\n",
            "y_true: 1 (real sentence) -- y_pred: 0.97\n",
            "   SAPLMA infer on single tokens: \u001b[0mAugust\u001b[0m\u001b[0m Hor\u001b[0m\u001b[0mch\u001b[0m\u001b[0m lived\u001b[0m\u001b[31m in\u001b[0m\u001b[31m Germany\u001b[0m\u001b[0m.\u001b[0m\n",
            "\n",
            "y_true: 1 (real sentence) -- y_pred: 0.86\n",
            "   SAPLMA infer on single tokens: \u001b[31mSp\u001b[0m\u001b[0mencer\u001b[0m\u001b[0m Silver\u001b[0m\u001b[0m invented\u001b[0m\u001b[0m the\u001b[0m\u001b[0m adhesive\u001b[0m\u001b[0m (\u001b[0m\u001b[0mused\u001b[0m\u001b[0m in\u001b[0m\u001b[31m Post\u001b[0m\u001b[0m-it\u001b[0m\u001b[0m notes\u001b[0m\u001b[0m).\u001b[0m\n",
            "\n",
            "y_true: 1 (real sentence) -- y_pred: 0.94\n",
            "   SAPLMA infer on single tokens: \u001b[31mThe\u001b[0m\u001b[0m sea\u001b[0m\u001b[0m turtle\u001b[0m\u001b[0m is\u001b[0m\u001b[0m a\u001b[0m\u001b[0m rept\u001b[0m\u001b[0mile\u001b[0m\u001b[0m.\u001b[0m\n",
            "\n",
            "y_true: 1 (real sentence) -- y_pred: 0.26\n",
            "   SAPLMA infer on single tokens: \u001b[33mE\u001b[0m\u001b[0mlish\u001b[0m\u001b[31ma\u001b[0m\u001b[0m Gray\u001b[0m\u001b[33m invented\u001b[0m\u001b[33m the\u001b[0m\u001b[31m tele\u001b[0m\u001b[0mgraph\u001b[0m\u001b[33m printer\u001b[0m\u001b[0m.\u001b[0m\n",
            "\n",
            "y_true: 1 (real sentence) -- y_pred: 0.78\n",
            "   SAPLMA infer on single tokens: \u001b[31mL\u001b[0m\u001b[0mil\u001b[0m\u001b[0mong\u001b[0m\u001b[31mwe\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[0m name\u001b[0m\u001b[31m of\u001b[0m\u001b[31m a\u001b[0m\u001b[0m city\u001b[0m\u001b[31m.\u001b[0m\n",
            "\n",
            "y_true: 1 (real sentence) -- y_pred: 0.79\n",
            "   SAPLMA infer on single tokens: \u001b[31mMap\u001b[0m\u001b[31muto\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[0m city\u001b[0m\u001b[31m in\u001b[0m\u001b[0m Moz\u001b[0m\u001b[0mambique\u001b[0m\n",
            "\n",
            "y_true: 1 (real sentence) -- y_pred: 0.96\n",
            "   SAPLMA infer on single tokens: \u001b[31mThe\u001b[0m\u001b[0m tap\u001b[0m\u001b[0mir\u001b[0m\u001b[0m has\u001b[0m\u001b[31m a\u001b[0m\u001b[0m habitat\u001b[0m\u001b[31m of\u001b[0m\u001b[0m forest\u001b[0m\u001b[0m.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    }
  ]
}