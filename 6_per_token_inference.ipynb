{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Per token inference\n",
        "\n",
        "Now that we have a model that works well with the last token,\n",
        "we want to explore how it behaves when used in a real-time environment,\n",
        "so that we may eventually have an hallucination probability inferred **while generating**, per single token."
      ],
      "metadata": {
        "id": "d62s8Z0tw39D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7riZzynJ_E9"
      },
      "source": [
        "# Imports, installations and declarations from previous notebooks\n",
        "\n",
        "This section can be skipped and collapsed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "8JXs-2fCJ_E-",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a34f143-52ea-413e-f269-009dbddd92ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.10.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.9)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.0)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.6)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.9)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "#@title Install missing dependencies\n",
        "!pip install wandb lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "3U1bN2Y-J_E_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "JH7N5rw6J_FA"
      },
      "outputs": [],
      "source": [
        "# If not in Colab, do some compatibility changes\n",
        "if not IN_COLAB:\n",
        "    DRIVE_PATH='.'\n",
        "    os.environ['HF_TOKEN'] = open('.hf_token').read().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fP6gb_MJ_FA",
        "outputId": "73f7c491-c70f-4548-c6ba-7e4c67da21dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content\n",
            "artifacts  drive  hallucination_detector  publicDataset  sample_data  wandb\n",
            "\n",
            "HF_TOKEN found\n",
            "WANDB_API_KEY found and set as env var\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Drive, if needed, and check the HF_TOKEN is set and accessible\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive, userdata\n",
        "\n",
        "    drive.mount('/content/drive', readonly=True)\n",
        "    DRIVE_PATH: str = '/content/drive/MyDrive/Final_Project/'\n",
        "    assert os.path.exists(DRIVE_PATH), 'Did you forget to create a shortcut in MyDrive named Final_Project this time as well? :('\n",
        "    !cp -R {DRIVE_PATH}/publicDataset .\n",
        "    !pwd\n",
        "    !ls\n",
        "    print()\n",
        "\n",
        "    assert userdata.get('HF_TOKEN'), 'Set up HuggingFace login secret properly in Colab!'\n",
        "    print('HF_TOKEN found')\n",
        "\n",
        "    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "    print('WANDB_API_KEY found and set as env var')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "GYKSjI-DJ_FA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d71a29b-ea44-46ec-eec0-4c6967b0cef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_ecdsa  known_hosts\n",
            "Cloning into '/content/AML-project'...\n",
            "remote: Enumerating objects: 418, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 418 (delta 41), reused 53 (delta 30), pack-reused 338 (from 1)\u001b[K\n",
            "Receiving objects: 100% (418/418), 2.12 MiB | 5.03 MiB/s, done.\n",
            "Resolving deltas: 100% (226/226), done.\n"
          ]
        }
      ],
      "source": [
        "#@title Clone the new updated Python files from GitHub, from master\n",
        "if IN_COLAB:\n",
        "  !mkdir -p /root/.ssh\n",
        "  !touch /root/.ssh/id_ecdsa\n",
        "\n",
        "  with open('/root/.ssh/id_ecdsa', 'w') as f:\n",
        "    git_ssh_private_key = \"\"\"\n",
        "        -----BEGIN OPENSSH PRIVATE KEY-----\n",
        "        b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\n",
        "        QyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQAAAAJhAtW8YQLVv\n",
        "        GAAAAAtzc2gtZWQyNTUxOQAAACCB3clOafi6fZaBgQCN29TVyJKNW/eVRXT4/B4MB28VQA\n",
        "        AAAEA6ARNr020VevD7mkC4GFBVqlTcZP7hvn8B3xi5LDvzYIHdyU5p+Lp9loGBAI3b1NXI\n",
        "        ko1b95VFdPj8HgwHbxVAAAAAEHNpbW9uZUBhcmNobGludXgBAgMEBQ==\n",
        "        -----END OPENSSH PRIVATE KEY-----\n",
        "    \"\"\"\n",
        "    f.write('\\n'.join([line.strip() for line in git_ssh_private_key.split('\\n') if line.strip() ]) + '\\n')\n",
        "\n",
        "  with open('/root/.ssh/known_hosts', 'w') as f:\n",
        "    f.write(\"github.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\\n\")\n",
        "    f.write(\"github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\\n\")\n",
        "    f.write(\"github.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\\n\")\n",
        "\n",
        "  !chmod 400 ~/.ssh/id_ecdsa ~/.ssh/known_hosts\n",
        "  !ls ~/.ssh\n",
        "\n",
        "  # Clone the repository\n",
        "  !rm -rf /content/AML-project\n",
        "  !git clone git@github.com:simonesestito/AML-project.git /content/AML-project\n",
        "  assert os.path.exists('/content/AML-project/.git'), 'Error cloning the repository. See logs above for details'\n",
        "  !rm -rf ./hallucination_detector && mv /content/AML-project/hallucination_detector .\n",
        "  !rm -rf /content/AML-project  # We don't need the Git repo anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "Ly5uv1XmJ_FB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "575e9e37-294d-4b20-cf48-504930c2f0ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 1\n",
        "%aimport hallucination_detector\n",
        "import hallucination_detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMraWvbuJ_FB"
      },
      "source": [
        "# Initialize Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {
        "id": "XiCsNDlgJ_FC"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import lightning as pl\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from hallucination_detector.llama import LlamaInstruct, LlamaPrompt\n",
        "from hallucination_detector.dataset import StatementDataModule\n",
        "from hallucination_detector.extractor import LlamaHiddenStatesExtractor, WeightedMeanReduction, AttentionAwareWeightedMeanReduction\n",
        "from hallucination_detector.classifier import OriginalSAPLMAClassifier, LightningHiddenStateSAPLMA, EnhancedSAPLMAClassifier\n",
        "from hallucination_detector.utils import try_to_overfit, plot_weight_matrix, classificator_evaluation\n",
        "import wandb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "GEjB3pgZJ_FC"
      },
      "outputs": [],
      "source": [
        "llama = LlamaInstruct()\n",
        "assert not IN_COLAB or llama.device.type == 'cuda', 'The model should be running on a GPU. On CPU, it is impossible to run'\n",
        "\n",
        "if llama.device.type == 'cpu':\n",
        "    print('WARNING: You are running an LLM on the CPU. Beware of the long inference times! Use it ONLY FOR SMALL tests, like very small tests.', file=sys.stderr, flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize trained `SAPLMAClassifier`"
      ],
      "metadata": {
        "id": "P4QtrsCbxV8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saplma_artifact_id = 'aml-2324-project/llama-hallucination-detector/attention-aware-weighted-tokens-architecture-hc7ivucr:best'\n",
        "\n",
        "run = wandb.init()\n",
        "artifact = run.use_artifact(saplma_artifact_id, type='model')\n",
        "artifact_dir = artifact.download()\n",
        "artifact_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "oYI9CJK6xgf8",
        "outputId": "74cf96b3-5e78-49af-f9f2-32740e07ef3c"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:g7wd39g3) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">still-vortex-1</strong> at: <a href='https://wandb.ai/sestito-1937764/uncategorized/runs/g7wd39g3' target=\"_blank\">https://wandb.ai/sestito-1937764/uncategorized/runs/g7wd39g3</a><br/> View project at: <a href='https://wandb.ai/sestito-1937764/uncategorized' target=\"_blank\">https://wandb.ai/sestito-1937764/uncategorized</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241209_094201-g7wd39g3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:g7wd39g3). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241209_113933-ekg67m18</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sestito-1937764/uncategorized/runs/ekg67m18' target=\"_blank\">azure-wave-2</a></strong> to <a href='https://wandb.ai/sestito-1937764/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sestito-1937764/uncategorized' target=\"_blank\">https://wandb.ai/sestito-1937764/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sestito-1937764/uncategorized/runs/ekg67m18' target=\"_blank\">https://wandb.ai/sestito-1937764/uncategorized/runs/ekg67m18</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/artifacts/attention-aware-weighted-tokens-architecture-hc7ivucr:v10'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {artifact_dir}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBrWUzSlxwV8",
        "outputId": "7063a4bd-e9a5-4a64-883d-1ef9add24d18"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saplma = LightningHiddenStateSAPLMA.load_from_checkpoint(\n",
        "    os.path.join(artifact_dir, 'model.ckpt'),\n",
        "    llama=llama,\n",
        "    saplma_classifier=OriginalSAPLMAClassifier(),\n",
        "    reduction=AttentionAwareWeightedMeanReduction(),\n",
        ").eval()"
      ],
      "metadata": {
        "id": "BZGI4RK2zg3q"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "_mHDKiLs0dIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hallucination_detector.extractor.tokenizer import tokenize_prompts_fixed_length"
      ],
      "metadata": {
        "id": "REHp5NYy1Cr3"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "datamodule = StatementDataModule(batch_size=batch_size, drive_path='publicDataset')\n",
        "datamodule.prepare_data()\n",
        "print(f'Found {len(datamodule.full_dataset)} samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZt8dFmS0ce3",
        "outputId": "ad6db537-36a3-4c69-8718-8ac379e7534e"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading file: cities_true_false.csv\n",
            "Loading file: facts_true_false.csv\n",
            "Loading file: animals_true_false.csv\n",
            "Loading file: elements_true_false.csv\n",
            "Loading file: inventions_true_false.csv\n",
            "Loading file: companies_true_false.csv\n",
            "Loading file: generated_true_false.csv\n",
            "Found 6330 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample = datamodule.full_dataset[980]\n",
        "random_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PSlL-VF0a0h",
        "outputId": "5cf2b62a-909d-45e9-caf0-37bf45aa6b82"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Tokyo is a name of a country.', tensor(0), 'cities_true_false')"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_prefix_suffix_from_tokens(tokens: torch.Tensor) -> torch.Tensor:\n",
        "  random_user_input = 'y6BabNgCyZf3A9XC3d1Qr'\n",
        "\n",
        "  # Extract the strings for prefix and suffix, that are added by LlamaPrompt\n",
        "  full_llama_prompt = str(LlamaPrompt(random_user_input))\n",
        "  prefix_len = full_llama_prompt.index(random_user_input)\n",
        "  suffix_len = len(full_llama_prompt) - prefix_len - len(random_user_input)\n",
        "  prefix, suffix = full_llama_prompt[:prefix_len], full_llama_prompt[-suffix_len:]\n",
        "\n",
        "  # Count how many tokens do they require\n",
        "  prefix_len = llama.tokenizer([prefix], return_tensors='pt').input_ids.ravel().size(0)\n",
        "  suffix_len = llama.tokenizer([suffix], return_tensors='pt').input_ids.ravel().size(0)\n",
        "\n",
        "  # Remove the tokens that are not part of the user input we want to analyze\n",
        "  tokens = tokens[prefix_len:-suffix_len+1]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "QCPs6e2_N285"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test_single_tokens_with_saplma_inference(statement: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    tokenized_sample = tokenize_prompts_fixed_length(llama, statement)\n",
        "    token_ids, attn_mask = tokenized_sample.input_ids.squeeze(), tokenized_sample.attention_mask.squeeze()\n",
        "    real_token_ids = token_ids[attn_mask == 1]\n",
        "\n",
        "    model_dtype = next(saplma.saplma_classifier.parameters()).dtype\n",
        "\n",
        "    hidden_states = saplma.hidden_states_extractor.extract_input_hidden_states_for_layer(\n",
        "        prompt=statement,\n",
        "        for_layer=11,\n",
        "    ).detach().to(model_dtype)[0]   # returns [70, 2048] = [TOKENS, EMBEDDING_DIM]\n",
        "    assert hidden_states.shape == (70, 2048)\n",
        "\n",
        "    # Consider each token as a sample in a batch\n",
        "    # ignoring the ones that are not part of the real input statement\n",
        "    real_hidden_states = hidden_states[attn_mask == 1]\n",
        "    assert len(real_hidden_states.shape) == 2\n",
        "    assert real_hidden_states.size(1) == 2048\n",
        "\n",
        "    each_token_classification = saplma.saplma_classifier(real_hidden_states)\n",
        "\n",
        "    # Remove the tokens that are not part of the user input we want to analyze\n",
        "    each_token_classification = remove_prefix_suffix_from_tokens(each_token_classification)\n",
        "    real_token_ids = remove_prefix_suffix_from_tokens(real_token_ids)\n",
        "\n",
        "    return each_token_classification, real_token_ids\n",
        "\n",
        "\n",
        "print(random_sample[0])\n",
        "each_token_classification, real_token_ids = test_single_tokens_with_saplma_inference(random_sample[0])\n",
        "\n",
        "for hallucination_probability, token in zip(each_token_classification, real_token_ids):\n",
        "    print(f'{hallucination_probability.item():>6.1%}: {llama.tokenizer.decode(token)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T73EpsT612ed",
        "outputId": "c960550e-d267-4754-9efc-ebcb645724ce"
      },
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokyo is a name of a country.\n",
            "  7.2%: Tok\n",
            "  1.0%: yo\n",
            "  5.5%:  is\n",
            "  0.0%:  a\n",
            "  4.3%:  name\n",
            "  0.2%:  of\n",
            "  0.0%:  a\n",
            "  0.0%:  country\n",
            "  0.0%: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try with gradients\n",
        "\n",
        "A similar approach to gradcam"
      ],
      "metadata": {
        "id": "MgGjqhg0-Wv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_input_hidden_states_for_layers(prompt, for_layers: set[int]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Given a batch of prompts, with length BATCH_SIZE,\n",
        "    extract the hidden states for the L requested layers.\n",
        "\n",
        "    The output tensor will have shape [BATCH_SIZE, L, SEQ_LEN, TOKEN_DIM].\n",
        "    SEQ_LEN is the length of the input sequence, which is the same for all prompts in the batch, fixed to 70.\n",
        "    TOKEN_DIM is the dimension of the hidden states, which is the same for all layers in the model, fixed to 2048.\n",
        "    \"\"\"\n",
        "    if isinstance(for_layers, list) or isinstance(for_layers, tuple):\n",
        "        for_layers = set(for_layers)\n",
        "    assert isinstance(for_layers, set), f\"Expected for_layers to be a set. Found: {type(for_layers)}\"\n",
        "\n",
        "    max_layers = len(llama.iter_layers())\n",
        "    assert all(0 <= layer < max_layers for layer in for_layers), f\"Expected all layers to be in range [0, {max_layers}). Found: {for_layers}\"\n",
        "\n",
        "    hidden_states = []\n",
        "\n",
        "    def _collect_hidden_states(layer_idx: int):\n",
        "        def _hook(module, inputs, outputs):\n",
        "            assert isinstance(outputs, tuple), f\"Expected outputs to be a tuple. Found: {type(outputs)}\"\n",
        "            assert len(outputs) >= 1, f\"Expected outputs to have 1+ elements. Found: {len(outputs)}\"\n",
        "\n",
        "            hidden_state = outputs[0]\n",
        "            assert isinstance(hidden_state, torch.Tensor), f\"Expected hidden_state to be a torch.Tensor. Found: {type(hidden_state)}\"\n",
        "            assert hidden_state.size(1) == 70 and hidden_state.size(2) == 2048, f\"Expected hidden_state to have shape (?, 70, 2048). Found: {hidden_state.shape}\"\n",
        "            hidden_states.append(hidden_state)\n",
        "        return _hook\n",
        "\n",
        "    llama.unregister_all_hooks()\n",
        "    for layer_idx, decoder_layer in enumerate(llama.iter_layers()):\n",
        "        if layer_idx in for_layers:\n",
        "            llama.register_hook(decoder_layer, _collect_hidden_states(layer_idx))\n",
        "\n",
        "    inputs = tokenize_prompts_fixed_length(llama, prompt)\n",
        "    embedded_inputs = llama.model.get_input_embeddings()(inputs.input_ids)\n",
        "    embedded_inputs = embedded_inputs.clone().detach().requires_grad_(True)\n",
        "    embedded_inputs.retain_grad()\n",
        "    _ = llama.model(\n",
        "        inputs_embeds=embedded_inputs,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        **{\n",
        "            \"max_length\": None,\n",
        "            \"max_new_tokens\": 1,\n",
        "            \"num_return_sequences\": 1,\n",
        "            # We are collecting hidden_states in a more fine-grained way with hooks\n",
        "            \"output_attentions\": False,\n",
        "            \"output_hidden_states\": False,\n",
        "            \"return_dict_in_generate\": False,\n",
        "        }\n",
        "    )\n",
        "    llama.unregister_all_hooks()\n",
        "\n",
        "    # Now, hidden_states are a list of tensors, each tensor representing the hidden_state for a layer we requested\n",
        "    return embedded_inputs, torch.stack(hidden_states).transpose(0, 1)"
      ],
      "metadata": {
        "id": "Osk29A89EPUG"
      },
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tokens_with_grad(statement: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    tokenized_sample = tokenize_prompts_fixed_length(llama, statement)\n",
        "    token_ids, attn_mask = tokenized_sample.input_ids.squeeze(), tokenized_sample.attention_mask.squeeze()\n",
        "    real_token_ids = token_ids[attn_mask == 1]\n",
        "\n",
        "    # Do a forward pass, with also returning the input embeddings (with requires_grad=True)\n",
        "    embedded_inputs, hidden_states = extract_input_hidden_states_for_layers(\n",
        "        statement,\n",
        "        for_layers={11},\n",
        "    )\n",
        "    hidden_states = hidden_states.squeeze(0, 1).to(torch.float32)\n",
        "    assert hidden_states.shape == (70, 2048)\n",
        "\n",
        "    saplma_input = hidden_states[64].unsqueeze(0)\n",
        "    assert saplma_input.shape == (1, 2048)\n",
        "    prediction = saplma.saplma_classifier(saplma_input)\n",
        "    print(f'Realistic probability (inferred): {prediction.item():.1%}')\n",
        "    (5 * (prediction + 1)).sum().backward()  # Compute gradients on the input\n",
        "\n",
        "    # Reduce the gradients on the input embeddings, summing up all dimensions of every token\n",
        "    embedded_inputs_grads = embedded_inputs.grad[0].sum(dim=1)[attn_mask == 1]\n",
        "\n",
        "    # Remove the tokens that are not part of the user input we want to analyze\n",
        "    embedded_inputs_grads = remove_prefix_suffix_from_tokens(embedded_inputs_grads)\n",
        "    real_token_ids = remove_prefix_suffix_from_tokens(real_token_ids)\n",
        "    assert embedded_inputs_grads.shape == real_token_ids.shape\n",
        "    return F.softmax(embedded_inputs_grads, dim=0), real_token_ids\n",
        "\n",
        "\n",
        "print(random_sample[0])\n",
        "embedded_inputs_grads, real_token_ids = test_tokens_with_grad(random_sample[0])\n",
        "\n",
        "for hallucination_probability, token in zip(embedded_inputs_grads, real_token_ids):\n",
        "    print(f'{hallucination_probability.item():>6.1%}: {llama.tokenizer.decode(token)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYy6O1SV-e7O",
        "outputId": "f81a48d4-c8c8-4fc4-b7ed-62df59362c1b"
      },
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokyo is a name of a country.\n",
            "Realistic probability (inferred): 0.0%\n",
            " 11.3%: Tok\n",
            " 10.8%: yo\n",
            " 11.2%:  is\n",
            " 11.1%:  a\n",
            " 11.1%:  name\n",
            " 11.1%:  of\n",
            " 11.2%:  a\n",
            " 11.1%:  country\n",
            " 11.1%: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do a few more tests\n",
        "\n",
        "Compare these 2 approaches"
      ],
      "metadata": {
        "id": "yF7pBGn1OH14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tokens(sample: tuple, strategy, verbose=True) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    statement, is_hallucination, _ = sample\n",
        "    if verbose:\n",
        "      print('Using strategy:', strategy.__name__)\n",
        "    probabilities, token_ids = strategy(statement)\n",
        "\n",
        "    # Normalize the probabilities\n",
        "    probs_mean = torch.mean(probabilities)\n",
        "    probs_std = torch.std(probabilities)\n",
        "    probabilities = (probabilities - probs_mean) / probs_std\n",
        "\n",
        "    temperature = 0.9\n",
        "    probabilities = F.softmax(probabilities / temperature, dim=0)\n",
        "\n",
        "    color_thresholds = [\n",
        "        (0.35, '\\033[0m'),  # Neutral\n",
        "        (0.65, '\\033[33m'), # Yellow\n",
        "        (1.01, '\\033[31m'), # Red\n",
        "    ]\n",
        "\n",
        "    for hallucination_probability, token in zip(probabilities, token_ids):\n",
        "        if hallucination_probability.isnan():\n",
        "            hallucination_probability = 1.0\n",
        "        color_for_probability = next(style for threshold, style in color_thresholds if hallucination_probability < threshold)\n",
        "        print(f'{color_for_probability}{llama.tokenizer.decode(token)}\\033[0m', end='')\n",
        "    print(f'\\nGround truth: {is_hallucination}\\n')"
      ],
      "metadata": {
        "id": "Y0iY3u2SOJn_"
      },
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_true_sample = datamodule.full_dataset[1000]\n",
        "random_false_sample = datamodule.full_dataset[2000]\n",
        "\n",
        "test_tokens(random_true_sample, strategy=test_tokens_with_grad)\n",
        "test_tokens(random_false_sample, strategy=test_tokens_with_grad)\n",
        "\n",
        "test_tokens(random_true_sample, strategy=test_single_tokens_with_saplma_inference)\n",
        "test_tokens(random_false_sample, strategy=test_single_tokens_with_saplma_inference)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpL24bZGNoIz",
        "outputId": "c64e4c49-194b-4269-d814-263a8fbdd8df"
      },
      "execution_count": 408,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using strategy: test_tokens_with_grad\n",
            "Realistic probability (inferred): 0.3%\n",
            "\u001b[0mG\u001b[0m\u001b[33mren\u001b[0m\u001b[0mada\u001b[0m\u001b[0m is\u001b[0m\u001b[0m a\u001b[0m\u001b[0m name\u001b[0m\u001b[0m of\u001b[0m\u001b[0m a\u001b[0m\u001b[0m city\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Using strategy: test_tokens_with_grad\n",
            "Realistic probability (inferred): 100.0%\n",
            "\u001b[0mA\u001b[0m\u001b[0m group\u001b[0m\u001b[0m of\u001b[0m\u001b[0m wolves\u001b[0m\u001b[0m is\u001b[0m\u001b[0m called\u001b[0m\u001b[0m a\u001b[0m\u001b[31m pack\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 1\n",
            "\n",
            "Using strategy: test_single_tokens_with_saplma_inference\n",
            "\u001b[0mG\u001b[0m\u001b[31mren\u001b[0m\u001b[0mada\u001b[0m\u001b[0m is\u001b[0m\u001b[0m a\u001b[0m\u001b[0m name\u001b[0m\u001b[0m of\u001b[0m\u001b[0m a\u001b[0m\u001b[0m city\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Using strategy: test_single_tokens_with_saplma_inference\n",
            "\u001b[0mA\u001b[0m\u001b[0m group\u001b[0m\u001b[0m of\u001b[0m\u001b[0m wolves\u001b[0m\u001b[0m is\u001b[0m\u001b[0m called\u001b[0m\u001b[0m a\u001b[0m\u001b[0m pack\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_true_sample = datamodule.full_dataset[1500]\n",
        "random_false_sample = datamodule.full_dataset[1701]\n",
        "\n",
        "test_tokens(random_true_sample, strategy=test_tokens_with_grad)\n",
        "test_tokens(random_false_sample, strategy=test_tokens_with_grad)\n",
        "\n",
        "test_tokens(random_true_sample, strategy=test_single_tokens_with_saplma_inference)\n",
        "test_tokens(random_false_sample, strategy=test_single_tokens_with_saplma_inference)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRSoG0I0R-Bh",
        "outputId": "62b288db-97ae-41c1-c486-0959a616e878"
      },
      "execution_count": 409,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using strategy: test_tokens_with_grad\n",
            "Realistic probability (inferred): 99.9%\n",
            "\u001b[0mRain\u001b[0m\u001b[0mbows\u001b[0m\u001b[0m form\u001b[0m\u001b[0m when\u001b[0m\u001b[0m light\u001b[0m\u001b[0m refr\u001b[0m\u001b[0macts\u001b[0m\u001b[0m through\u001b[0m\u001b[0m water\u001b[0m\u001b[0m dro\u001b[0m\u001b[0mplets\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 1\n",
            "\n",
            "Using strategy: test_tokens_with_grad\n",
            "Realistic probability (inferred): 0.8%\n",
            "\u001b[0mThe\u001b[0m\u001b[0m Earth\u001b[0m\u001b[0m's\u001b[0m\u001b[0m t\u001b[0m\u001b[0mides\u001b[0m\u001b[0m are\u001b[0m\u001b[0m primarily\u001b[0m\u001b[0m caused\u001b[0m\u001b[0m by\u001b[0m\u001b[0m the\u001b[0m\u001b[0m rep\u001b[0m\u001b[0mulsive\u001b[0m\u001b[0m push\u001b[0m\u001b[0m of\u001b[0m\u001b[0m the\u001b[0m\u001b[31m sun\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Using strategy: test_single_tokens_with_saplma_inference\n",
            "\u001b[0mRain\u001b[0m\u001b[0mbows\u001b[0m\u001b[0m form\u001b[0m\u001b[0m when\u001b[0m\u001b[0m light\u001b[0m\u001b[0m refr\u001b[0m\u001b[0macts\u001b[0m\u001b[0m through\u001b[0m\u001b[0m water\u001b[0m\u001b[0m dro\u001b[0m\u001b[0mplets\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 1\n",
            "\n",
            "Using strategy: test_single_tokens_with_saplma_inference\n",
            "\u001b[0mThe\u001b[0m\u001b[0m Earth\u001b[0m\u001b[0m's\u001b[0m\u001b[0m t\u001b[0m\u001b[33mides\u001b[0m\u001b[0m are\u001b[0m\u001b[0m primarily\u001b[0m\u001b[0m caused\u001b[0m\u001b[0m by\u001b[0m\u001b[0m the\u001b[0m\u001b[0m rep\u001b[0m\u001b[0mulsive\u001b[0m\u001b[0m push\u001b[0m\u001b[0m of\u001b[0m\u001b[0m the\u001b[0m\u001b[0m sun\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick 10 random samples from full_dataset\n",
        "false_sentences_indexes = [\n",
        "    sample for sample in datamodule.full_dataset if not sample[1]\n",
        "]\n",
        "random_samples = random.sample(false_sentences_indexes, 10)\n",
        "\n",
        "for sample in random_samples:\n",
        "    test_tokens(sample, strategy=test_tokens_with_grad, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOxiMgNaTArZ",
        "outputId": "8789bb9a-8a4e-44a2-cd1c-a1d35f4e525c"
      },
      "execution_count": 410,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Realistic probability (inferred): 52.6%\n",
            "\u001b[0mJohn\u001b[0m\u001b[0m Amb\u001b[0m\u001b[0mrose\u001b[0m\u001b[0m Fleming\u001b[0m\u001b[0m invented\u001b[0m\u001b[0m the\u001b[0m\u001b[31m Band\u001b[0m\u001b[0m-A\u001b[0m\u001b[0mid\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Realistic probability (inferred): 0.0%\n",
            "\u001b[0mSh\u001b[0m\u001b[0manghai\u001b[0m\u001b[0m P\u001b[0m\u001b[0mud\u001b[0m\u001b[0mong\u001b[0m\u001b[0m Development\u001b[0m\u001b[0m Bank\u001b[0m\u001b[0m operates\u001b[0m\u001b[0m in\u001b[0m\u001b[0m the\u001b[0m\u001b[0m industry\u001b[0m\u001b[0m of\u001b[0m\u001b[0m Banking\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Realistic probability (inferred): 99.6%\n",
            "\u001b[0mHar\u001b[0m\u001b[33mare\u001b[0m\u001b[0m is\u001b[0m\u001b[0m a\u001b[0m\u001b[0m city\u001b[0m\u001b[0m in\u001b[0m\u001b[0m Falk\u001b[0m\u001b[0mland\u001b[0m\u001b[0m Islands\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Realistic probability (inferred): 0.0%\n",
            "\u001b[0mThe\u001b[0m\u001b[0m ot\u001b[0m\u001b[0mter\u001b[0m\u001b[0m has\u001b[0m\u001b[0m long\u001b[0m\u001b[0m ears\u001b[0m\u001b[0m for\u001b[0m\u001b[0m detecting\u001b[0m\u001b[31m predators\u001b[0m\u001b[0m and\u001b[0m\u001b[0m strong\u001b[0m\u001b[0m hind\u001b[0m\u001b[0m legs\u001b[0m\u001b[0m for\u001b[0m\u001b[0m escaping\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Realistic probability (inferred): 0.0%\n",
            "\u001b[0mAs\u001b[0m\u001b[0munc\u001b[0m\u001b[0miÃ³n\u001b[0m\u001b[0m is\u001b[0m\u001b[0m a\u001b[0m\u001b[0m name\u001b[0m\u001b[0m of\u001b[0m\u001b[0m a\u001b[0m\u001b[0m country\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Realistic probability (inferred): 33.3%\n",
            "\u001b[0mToronto\u001b[0m\u001b[0m-D\u001b[0m\u001b[0momin\u001b[0m\u001b[0mion\u001b[0m\u001b[0m Bank\u001b[0m\u001b[0m engages\u001b[0m\u001b[0m in\u001b[0m\u001b[0m the\u001b[0m\u001b[0m provision\u001b[0m\u001b[0m of\u001b[0m\u001b[0m financial\u001b[0m\u001b[31m products\u001b[0m\u001b[0m and\u001b[0m\u001b[0m services\u001b[0m\u001b[0m..\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Realistic probability (inferred): 0.0%\n",
            "\u001b[0mBank\u001b[0m\u001b[0m of\u001b[0m\u001b[33m Communications\u001b[0m\u001b[0m operates\u001b[0m\u001b[0m in\u001b[0m\u001b[0m the\u001b[0m\u001b[0m industry\u001b[0m\u001b[0m of\u001b[0m\u001b[0m Technology\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Realistic probability (inferred): 61.3%\n",
            "\u001b[0mCarlos\u001b[0m\u001b[0m Gl\u001b[0m\u001b[0midden\u001b[0m\u001b[0m invented\u001b[0m\u001b[0m the\u001b[0m\u001b[31m ATM\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Realistic probability (inferred): 21.0%\n",
            "\u001b[0mHumans\u001b[0m\u001b[0m do\u001b[0m\u001b[0m not\u001b[0m\u001b[0m use\u001b[0m\u001b[31m their\u001b[0m\u001b[0m brains\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n",
            "Realistic probability (inferred): 0.1%\n",
            "\u001b[0mZ\u001b[0m\u001b[0minc\u001b[0m\u001b[0m appears\u001b[0m\u001b[0m in\u001b[0m\u001b[0m its\u001b[0m\u001b[0m standard\u001b[0m\u001b[0m state\u001b[0m\u001b[0m as\u001b[0m\u001b[0m Liquid\u001b[0m\u001b[0m.\u001b[0m\n",
            "Ground truth: 0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}